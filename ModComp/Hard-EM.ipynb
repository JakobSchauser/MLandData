{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The factor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "M = 1000\n",
    "# Random parameters\n",
    "beta0 = np.random.normal(0, 0.5, size=n)\n",
    "beta1 = np.random.normal(0, 1, size=n) + 1\n",
    "sigma = abs(np.random.normal(0, 1, size=n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.normal(0, 1, size=M)\n",
    "x = np.zeros(shape=(M, n))\n",
    "\n",
    "epsilon = np.random.normal(0, 1, size=(M, n))\n",
    "\n",
    "for k in range(n):\n",
    "    x[:, k] = beta0[k] + beta1[k] * z + sigma[k] * epsilon[:, k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0 # Try different k\n",
    "print(beta0[k], beta1[k], sigma[k])\n",
    "plt.scatter(z, x[:, k], alpha = 0.1)\n",
    "plt.plot(np.array([-4, 4]), beta0[k] + beta1[k] * np.array([-4, 4]), 'r')     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "The function `learn_beta` below learns all the beta-parameters from complete observations using regression. You could, of course, do that also by sklearn or stats.linregress, but one benefit of the implementation below is that it works directly for several targets, $X_1, \\ldots, X_n$, directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression with many responses\n",
    "def learn_beta(x, z):\n",
    "    U = np.stack((np.ones_like(z), z), 1)\n",
    "    Ut = np.transpose(U)\n",
    "    Sigma = Ut @ U\n",
    "    Sigma_inv =  np.linalg.inv(Sigma)\n",
    "    beta = Sigma_inv @ (Ut @ x)\n",
    "    return(beta[0, :], beta[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0_hat, beta1_hat = learn_beta(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison with true parameters\n",
    "beta0_hat, beta0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison with true parameters\n",
    "beta1_hat, beta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The inference algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the information parameters, the conditional distribution of $Z$ given $\\mathbf{X} = \\mathbf{x}$ has parameters\n",
    "\\begin{align*}\n",
    "J & =  \\sum_{k=1}^n \\frac{\\beta_{1,k}^2}{\\sigma_k^2} \\\\\n",
    "h & =  \\sum_{k=1}^n \\frac{(x_k - \\beta_{0,k})\\beta_{1,k}}{\\sigma_k^2}\n",
    "\\end{align*}\n",
    "and the conditional mean and variance are simple to compute from these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference of z for a single x\n",
    "def z_inf(x, beta0, beta1, sigma):\n",
    "    J = np.sum((beta1 / sigma) ** 2)\n",
    "    h = np.sum((x - beta0) * beta1 / (sigma ** 2))\n",
    "    return((h / J, 1 / J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "print(z[m])\n",
    "z_inf(x[m, :], beta0, beta1, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_hat(x, beta0, beta1, sigma):\n",
    "    M = x.shape[0]\n",
    "    z_hats = np.zeros(shape=M)\n",
    "    for m in range(M):\n",
    "        z_hats[m] = z_inf(x[m, :], beta0, beta1, sigma)[0]\n",
    "    return(z_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the inference algorithm produces sensible predictions\n",
    "z_hats = z_hat(x, beta0, beta1, sigma)\n",
    "plt.scatter(z, z_hats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hard-assignment EM algorithm\n",
    "\n",
    "The algorithm is implemented as a simple iteration ($N$ times) of prediction by the inference algorithm given current parameters and updating the parameters by learning the beta-parameters and then the sigma-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_EM(x, beta0_init, beta1_init, sigma_init, N=10):\n",
    "    M, n = x.shape\n",
    "    beta0 = beta0_init\n",
    "    beta1 = beta1_init\n",
    "    sigma = sigma_init\n",
    "    for i in range(N):\n",
    "        z_hats = z_hat(x, beta0, beta1, sigma)\n",
    "        beta0, beta1 = learn_beta(x, z_hats)\n",
    "        for k in range(n):\n",
    "            sigma[k] = np.sqrt(np.sum((x[:, k] - beta0[k] - beta1[k] * z) ** 2) / M)\n",
    "    return(beta0, beta1, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The most difficult part is actually choosing the starting values of the algorithm. Here we use that the means of the $x$-s are equal to the $\\beta_0$-parameters and that the variances of the $x$-s is an upper bound on the $\\sigma$-parameters. This gives some sensible choices of those paramters. The $\\beta_1$-parameters are just (somewhat arbitrarily) set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_EM(x, np.mean(x, 0), np.ones(n), np.sqrt(np.var(x, 0)), N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should compare the above to the true parameters\n",
    "beta0, beta1, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to test if the implementation behaves reasonably is to start the \n",
    "# algorithm in the true parameters\n",
    "hard_EM(x, beta0, beta1, sigma, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "It is a good sign that the algorithm doesn't start to drift off when started in the true parameters. However, the hard assignment EM algorithm doesn't have well-understood convergence properties and it may converge to slightly different values of the parameters depending on small changes in the choice of starting values. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
