{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82204d8",
   "metadata": {},
   "source": [
    "## Project Group 2\n",
    "#### Gabrielle H. B. Madsen, Jakob H. Schauser, Martin Pries-Brøndberg & Monika Haubro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498c2c9",
   "metadata": {},
   "source": [
    "### Import af biblioteker\n",
    "Indledningsvis importerer vi relevante biblioteker til besvarelse af opgaverne.\n",
    "\n",
    "- Med import af *requests* biblioteket bliver det muligt for os at foretage HTTP forespørgsler. Det er i høj grad en nødvendighed i task 4, hvor vi bliver bedt om at scrape fra et websted.\n",
    "\n",
    "- Vi har ligeledes importeret *BeautifulSoup4* biblioteket med det formål at scrape fra et website. Med dette kan vi mere præcist udtrække data fra HTML. Dette relaterer sig dermed til task 4.\n",
    "\n",
    "- I forbindelse med task 4 har vi valgt at importere *tdqm* - en progress bar, da det kan være en langsommelig proces at hente data fra et website.\n",
    "\n",
    "- Sidst har vi valgt at importere *pandas*, der tilbyder værktøjer til at undersøge, analysere og manipulere data. Det er derfor noget, vi anvender i både task 2, 3 og 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1340b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup  \n",
    "from tqdm.notebook import tqdm \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f04b9e",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "<i>Bemærk: Undersøgelse og opdagelse af datasættet fra Task 3 ligger i dette tilfælde sammen med Task 2 grundet overlap af opgaverne. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681b3c0",
   "metadata": {},
   "source": [
    "Vi starter med at indlæse selve datasættet ved hjælp af pakken *pandas*. <br>\n",
    "Herefter kalder vi head() for at få et overblik over, hvordan datasættet ser ud.\n",
    "\n",
    "Vi observerer, at der findes NaN (= Not a Number), hvilket er med i vores overvejelser senere, når der skal fjernes irrelevante kolonner fra datasættet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "23ed79a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/church-congregation-brings-gift...</td>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Church Congregation Brings Gift to Waitresses ...</td>\n",
       "      <td>Ruth Harris</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/awakening-start-here/...</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>Zurich Times</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>cnnnext.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://www.cnnnext.com/video/18526/never-hike-...</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Never Hike Alone - A Friday the 13th Fan Film ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film  ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>768</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/elusive-alien-of-the-sea-caught...</td>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Elusive ‘Alien Of The Sea ‘ Caught By Scientis...</td>\n",
       "      <td>Alexander Smith</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>791</td>\n",
       "      <td>bipartisanreport.com</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>http://bipartisanreport.com/2018/01/21/trumps-...</td>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>2018-01-25 16:17:44.789555</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Trump’s Genius Poll Is Complete &amp; The Results ...</td>\n",
       "      <td>Gloria Christie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   id                domain        type  \\\n",
       "0           0  141               awm.com  unreliable   \n",
       "1           1  256     beforeitsnews.com        fake   \n",
       "2           2  700           cnnnext.com  unreliable   \n",
       "3           3  768               awm.com  unreliable   \n",
       "4           4  791  bipartisanreport.com   clickbait   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://awm.com/church-congregation-brings-gift...   \n",
       "1  http://beforeitsnews.com/awakening-start-here/...   \n",
       "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
       "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
       "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sometimes the power of Christmas will make you...   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "3  When a rare shark was caught, scientists were ...   \n",
       "4  Donald Trump has the unnerving ability to abil...   \n",
       "\n",
       "                   scraped_at                 inserted_at  \\\n",
       "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
       "\n",
       "                   updated_at  \\\n",
       "0  2018-02-02 01:19:41.756664   \n",
       "1  2018-02-02 01:19:41.756664   \n",
       "2  2018-02-02 01:19:41.756664   \n",
       "3  2018-02-02 01:19:41.756664   \n",
       "4  2018-02-02 01:19:41.756664   \n",
       "\n",
       "                                               title          authors  \\\n",
       "0  Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
       "2  Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
       "3  Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
       "4  Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
       "\n",
       "   keywords meta_keywords                                   meta_description  \\\n",
       "0       NaN          ['']                                                NaN   \n",
       "1       NaN          ['']                                                NaN   \n",
       "2       NaN          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
       "3       NaN          ['']                                                NaN   \n",
       "4       NaN          ['']                                                NaN   \n",
       "\n",
       "  tags  summary  \n",
       "0  NaN      NaN  \n",
       "1  NaN      NaN  \n",
       "2  NaN      NaN  \n",
       "3  NaN      NaN  \n",
       "4  NaN      NaN  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url = \"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\"\n",
    "# file = urllib.requt.urlopen(url)\n",
    "df_fakenews = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "df_fakenews.head() # The first five rows of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64625785",
   "metadata": {},
   "source": [
    "Vi undersøger datasættet for unikke værdier og tomme rækker, hvorved vi kan få en forståelse for sammensætningen. Det er de første skridt i rensning/strukturering af vores data. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f38188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 12, 0, 0, 0, 0, 0, 0, 80, 250, 0, 196, 223, 250]\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# We define the amount of NaN values per column, in order to check how many values are missing\n",
    "id_NaN = df_fakenews['id'].isna().sum()\n",
    "domain_NaN = df_fakenews['domain'].isna().sum()\n",
    "type_NaN = df_fakenews['type'].isna().sum()\n",
    "url_NaN = df_fakenews['url'].isna().sum()\n",
    "content_NaN = df_fakenews['content'].isna().sum()\n",
    "scraped_at_NaN = df_fakenews['scraped_at'].isna().sum()\n",
    "inserted_at_NaN = df_fakenews['inserted_at'].isna().sum()\n",
    "updated_at_NaN = df_fakenews['updated_at'].isna().sum()\n",
    "title_NaN = df_fakenews['title'].isna().sum()\n",
    "authors_NaN = df_fakenews['authors'].isna().sum()\n",
    "keywords_NaN = df_fakenews['keywords'].isna().sum()\n",
    "meta_keywords_NaN = df_fakenews['meta_keywords'].isna().sum()\n",
    "meta_description_NaN = df_fakenews['meta_description'].isna().sum()\n",
    "tags_NaN = df_fakenews['tags'].isna().sum()\n",
    "summary_NaN = df_fakenews['summary'].isna().sum()\n",
    "\n",
    "list_of_NaN = [id_NaN, domain_NaN, type_NaN, url_NaN, content_NaN, scraped_at_NaN, inserted_at_NaN, updated_at_NaN, title_NaN, authors_NaN, keywords_NaN, meta_keywords_NaN, meta_description_NaN, tags_NaN, summary_NaN]\n",
    "\n",
    "print (list_of_NaN) # List containing the NaN count for every column\n",
    "print (len(df_fakenews)) # Total amount of rows in order to compare the amount of missing numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d62544",
   "metadata": {},
   "source": [
    "Herunder kalder vi nunique(), som bruges til at undersøge, hvor mange unikke værdier der er i hver kolonne. <br><br>\n",
    "Det bemærkes her at nogle kolonner er helt tomme eller kun har en enkelt værdi. De kolonner fjerner vi fra datasættet, da de ikke er relevante. En enkelt kolonne, scraped_at har to værdier, hvor den ene type var scraped 4 timer efter den anden type. Vi vurderede derfor, at kolonnen ikke er relevant, og den fjernes herefter.\n",
    "<br><br>\n",
    "I alt er kolonnerne scraped_at, inserted_at, updated_at, keywords og summary fjernet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e11126f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0          250\n",
       "id                  250\n",
       "domain               29\n",
       "type                 10\n",
       "url                 250\n",
       "content             239\n",
       "scraped_at            2\n",
       "inserted_at           1\n",
       "updated_at            1\n",
       "title               248\n",
       "authors             109\n",
       "keywords              0\n",
       "meta_keywords        36\n",
       "meta_description     51\n",
       "tags                 24\n",
       "summary               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique values per column\n",
    "df_fakenews.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e75438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-25 16:17:44.789555    197\n",
      "2018-01-25 20:13:50.426130     53\n",
      "Name: scraped_at, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We see that 'scraped_at' contains 2 different values\n",
    "print (df_fakenews['scraped_at'].value_counts()) # We look at what the different values are in order to check whether the column is relevant for the dataset or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "210b611d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/church-congregation-brings-gift...</td>\n",
       "      <td>Sometimes the power of Christmas will make you...</td>\n",
       "      <td>Church Congregation Brings Gift to Waitresses ...</td>\n",
       "      <td>Ruth Harris</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/awakening-start-here/...</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>AWAKENING OF 12 STRANDS of DNA – “Reconnecting...</td>\n",
       "      <td>Zurich Times</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>700</td>\n",
       "      <td>cnnnext.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://www.cnnnext.com/video/18526/never-hike-...</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film U...</td>\n",
       "      <td>Never Hike Alone - A Friday the 13th Fan Film ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Never Hike Alone: A Friday the 13th Fan Film  ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>768</td>\n",
       "      <td>awm.com</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>http://awm.com/elusive-alien-of-the-sea-caught...</td>\n",
       "      <td>When a rare shark was caught, scientists were ...</td>\n",
       "      <td>Elusive ‘Alien Of The Sea ‘ Caught By Scientis...</td>\n",
       "      <td>Alexander Smith</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>791</td>\n",
       "      <td>bipartisanreport.com</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>http://bipartisanreport.com/2018/01/21/trumps-...</td>\n",
       "      <td>Donald Trump has the unnerving ability to abil...</td>\n",
       "      <td>Trump’s Genius Poll Is Complete &amp; The Results ...</td>\n",
       "      <td>Gloria Christie</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   id                domain        type  \\\n",
       "0           0  141               awm.com  unreliable   \n",
       "1           1  256     beforeitsnews.com        fake   \n",
       "2           2  700           cnnnext.com  unreliable   \n",
       "3           3  768               awm.com  unreliable   \n",
       "4           4  791  bipartisanreport.com   clickbait   \n",
       "\n",
       "                                                 url  \\\n",
       "0  http://awm.com/church-congregation-brings-gift...   \n",
       "1  http://beforeitsnews.com/awakening-start-here/...   \n",
       "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
       "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
       "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sometimes the power of Christmas will make you...   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...   \n",
       "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
       "3  When a rare shark was caught, scientists were ...   \n",
       "4  Donald Trump has the unnerving ability to abil...   \n",
       "\n",
       "                                               title          authors  \\\n",
       "0  Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
       "1  AWAKENING OF 12 STRANDS of DNA – “Reconnecting...     Zurich Times   \n",
       "2  Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
       "3  Elusive ‘Alien Of The Sea ‘ Caught By Scientis...  Alexander Smith   \n",
       "4  Trump’s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
       "\n",
       "  meta_keywords                                   meta_description tags  \n",
       "0          ['']                                                NaN  NaN  \n",
       "1          ['']                                                NaN  NaN  \n",
       "2          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...  NaN  \n",
       "3          ['']                                                NaN  NaN  \n",
       "4          ['']                                                NaN  NaN  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We remove these five columns from the dataset\n",
    "drops = ['keywords', # Contains only NaN values\n",
    "        'summary', # Contains only NaN values\n",
    "        'inserted_at', # Contains identical entries\n",
    "        'updated_at', # Contains identical entries\n",
    "        'scraped_at'] # Contains only two values, where the second entry is four hours later then the first entry\n",
    "\n",
    "\n",
    "new_data = df_fakenews # We define new_data to be the dataset where we have removed the five columns above\n",
    "new_data.drop(columns = drops, inplace = True) # We remove the columns\n",
    "\n",
    "\n",
    "new_data.head() # The first five rows, to assure everything went as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52985d2",
   "metadata": {},
   "source": [
    "I det følgende undersøger vi de enkelte kolonner for deres datatype med det formål at sikre os, at det stemmer overens med vores forventning, og det viser sig netop, at dataen er importeret korrekt dvs. som hhv. int64 ift. id samt object ift. strings/mixed strings. Datatyperne er derfor uredigeret.\n",
    " <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a6289cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0           int64\n",
       "id                   int64\n",
       "domain              object\n",
       "type                object\n",
       "url                 object\n",
       "content             object\n",
       "title               object\n",
       "authors             object\n",
       "meta_keywords       object\n",
       "meta_description    object\n",
       "tags                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.dtypes # We take a look at the different datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c0199",
   "metadata": {},
   "source": [
    "Nedenfor undersøger vi dataen nærmere. Vi ser bl.a. at hvert domæne kun har én type (fake, political, trust etc.). Det indikerer, at typerne er labeled efter domæne og ikke efter artiklernes indhold. Denne observation stemmer overens med dokumentationen for datasættet: \"*Each article has been attributed the same label as the label associated with its domain*\" (https://github.com/several27/FakeNewsCorpus).\n",
    "\n",
    "Først får vi et overblik over antallet af hver label. Dernæst kan vi sammenligne det samlede antal af hvert label med antallet af labels for hvert domæne. Til sidst kigger vi på, hvor mange unikke labels, der er knyttet til det enkelte domæne. Dermed ses det tydeligt, at hvert domæne kun har en enkelt unik label.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9c368986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake          155\n",
       "conspiracy     31\n",
       "political      23\n",
       "unreliable      6\n",
       "bias            6\n",
       "junksci         6\n",
       "unknown         6\n",
       "reliable        3\n",
       "clickbait       1\n",
       "hate            1\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['type'].value_counts() # The count of the different entries in 'type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d4fa0838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain                     type      \n",
      "21stcenturywire.com        conspiracy      1\n",
      "alternet.org               political       2\n",
      "americanlookout.com        bias            1\n",
      "anonhq.com                 unreliable      1\n",
      "awarenessact.com           conspiracy      6\n",
      "awm.com                    unreliable      2\n",
      "barenakedislam.com         hate            1\n",
      "beforeitsnews.com          fake          155\n",
      "bipartisanreport.com       clickbait       1\n",
      "blackagendareport.com      unreliable      1\n",
      "breakpoint.org             unreliable      1\n",
      "breitbart.com              political       1\n",
      "canadafreepress.com        conspiracy     24\n",
      "charismanews.com           bias            1\n",
      "christianpost.com          reliable        3\n",
      "city-journal.org           political       2\n",
      "cnnnext.com                unreliable      1\n",
      "collectivelyconscious.net  junksci         1\n",
      "nationalreview.com         political       1\n",
      "naturalnews.com            junksci         5\n",
      "strategic-culture.org      unknown         4\n",
      "undergroundhealth.com      unknown         2\n",
      "unz.com                    bias            1\n",
      "vdare.com                  bias            2\n",
      "washingtonexaminer.com     political      17\n",
      "washingtonsblog.com        bias            1\n",
      "Name: id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "t = new_data.groupby(by=['domain','type'], sort=\"True\").id.count() # We group by domain and type, in order to look at type of domains\n",
    "print(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7321dd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain\n",
       "21stcenturywire.com          1\n",
       "alternet.org                 1\n",
       "washingtonsblog.com          1\n",
       "washingtonexaminer.com       1\n",
       "vdare.com                    1\n",
       "unz.com                      1\n",
       "undergroundhealth.com        1\n",
       "strategic-culture.org        1\n",
       "naturalnews.com              1\n",
       "nationalreview.com           1\n",
       "collectivelyconscious.net    1\n",
       "cnnnext.com                  1\n",
       "city-journal.org             1\n",
       "christianpost.com            1\n",
       "charismanews.com             1\n",
       "canadafreepress.com          1\n",
       "breitbart.com                1\n",
       "breakpoint.org               1\n",
       "blackagendareport.com        1\n",
       "bipartisanreport.com         1\n",
       "beforeitsnews.com            1\n",
       "barenakedislam.com           1\n",
       "awm.com                      1\n",
       "awarenessact.com             1\n",
       "anonhq.com                   1\n",
       "americanlookout.com          1\n",
       "wallstreetonparade.com       0\n",
       "willyloman.wordpress.com     0\n",
       "www.newsmax.com              0\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.groupby(['domain'])['type'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460cc6e",
   "metadata": {},
   "source": [
    "I de næste to blokke undersøger vi to konkrete domæner, hvoraf den ene har en enkelt label og den anden ingen labels. I første tilfælde bliver det netop tydeligt, at domænet faktisk kun har en type label. I det andet tilfælde kigger vi på et domæne, der ikke har en label knyttet til sig. Det kommer til udtryk ved typen NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6d6d9a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1      fake\n",
      "7      fake\n",
      "8      fake\n",
      "10     fake\n",
      "13     fake\n",
      "23     fake\n",
      "26     fake\n",
      "27     fake\n",
      "29     fake\n",
      "30     fake\n",
      "32     fake\n",
      "34     fake\n",
      "37     fake\n",
      "39     fake\n",
      "40     fake\n",
      "42     fake\n",
      "44     fake\n",
      "45     fake\n",
      "47     fake\n",
      "50     fake\n",
      "56     fake\n",
      "61     fake\n",
      "62     fake\n",
      "63     fake\n",
      "65     fake\n",
      "68     fake\n",
      "72     fake\n",
      "75     fake\n",
      "76     fake\n",
      "77     fake\n",
      "78     fake\n",
      "80     fake\n",
      "81     fake\n",
      "84     fake\n",
      "85     fake\n",
      "88     fake\n",
      "90     fake\n",
      "91     fake\n",
      "92     fake\n",
      "93     fake\n",
      "96     fake\n",
      "97     fake\n",
      "98     fake\n",
      "99     fake\n",
      "100    fake\n",
      "101    fake\n",
      "102    fake\n",
      "104    fake\n",
      "106    fake\n",
      "107    fake\n",
      "108    fake\n",
      "109    fake\n",
      "110    fake\n",
      "111    fake\n",
      "112    fake\n",
      "113    fake\n",
      "115    fake\n",
      "117    fake\n",
      "118    fake\n",
      "119    fake\n",
      "120    fake\n",
      "122    fake\n",
      "123    fake\n",
      "124    fake\n",
      "125    fake\n",
      "126    fake\n",
      "127    fake\n",
      "128    fake\n",
      "129    fake\n",
      "130    fake\n",
      "131    fake\n",
      "132    fake\n",
      "134    fake\n",
      "135    fake\n",
      "136    fake\n",
      "137    fake\n",
      "138    fake\n",
      "139    fake\n",
      "140    fake\n",
      "141    fake\n",
      "142    fake\n",
      "143    fake\n",
      "144    fake\n",
      "145    fake\n",
      "146    fake\n",
      "147    fake\n",
      "148    fake\n",
      "149    fake\n",
      "150    fake\n",
      "151    fake\n",
      "152    fake\n",
      "153    fake\n",
      "154    fake\n",
      "155    fake\n",
      "156    fake\n",
      "157    fake\n",
      "158    fake\n",
      "159    fake\n",
      "160    fake\n",
      "161    fake\n",
      "162    fake\n",
      "163    fake\n",
      "164    fake\n",
      "165    fake\n",
      "166    fake\n",
      "170    fake\n",
      "171    fake\n",
      "172    fake\n",
      "173    fake\n",
      "174    fake\n",
      "175    fake\n",
      "177    fake\n",
      "179    fake\n",
      "180    fake\n",
      "181    fake\n",
      "183    fake\n",
      "186    fake\n",
      "187    fake\n",
      "188    fake\n",
      "189    fake\n",
      "192    fake\n",
      "193    fake\n",
      "195    fake\n",
      "197    fake\n",
      "198    fake\n",
      "202    fake\n",
      "203    fake\n",
      "204    fake\n",
      "206    fake\n",
      "207    fake\n",
      "208    fake\n",
      "210    fake\n",
      "211    fake\n",
      "212    fake\n",
      "213    fake\n",
      "214    fake\n",
      "215    fake\n",
      "216    fake\n",
      "217    fake\n",
      "219    fake\n",
      "221    fake\n",
      "224    fake\n",
      "225    fake\n",
      "226    fake\n",
      "227    fake\n",
      "229    fake\n",
      "230    fake\n",
      "231    fake\n",
      "232    fake\n",
      "233    fake\n",
      "236    fake\n",
      "239    fake\n",
      "243    fake\n",
      "245    fake\n",
      "246    fake\n",
      "Name: type, dtype: object\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(new_data[\"type\"].loc[new_data[\"domain\"] == \"beforeitsnews.com\"]) # We choose a domain, beforeitsnews.com, and look at the entries for 'type', to see if it they're all labelled 'fake'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02236b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0     id           domain type  \\\n",
      "238         238  38549  www.newsmax.com  NaN   \n",
      "240         240  38616  www.newsmax.com  NaN   \n",
      "241         241  38652  www.newsmax.com  NaN   \n",
      "242         242  38736  www.newsmax.com  NaN   \n",
      "247         247  39477  www.newsmax.com  NaN   \n",
      "248         248  39550  www.newsmax.com  NaN   \n",
      "249         249  39558  www.newsmax.com  NaN   \n",
      "\n",
      "                                                   url  \\\n",
      "238  https://www.newsmax.com/newsfront/david-letter...   \n",
      "240  https://www.newsmax.com/newsfront/department-o...   \n",
      "241  https://www.newsmax.com/politics/tom-perez-sla...   \n",
      "242  https://www.newsmax.com/politics/chuck-schumer...   \n",
      "247  https://www.newsmax.com/politics/michael-hayde...   \n",
      "248  https://www.newsmax.com/newsfront/antonio-saba...   \n",
      "249  https://www.newsmax.com/newsfront/bill-clinton...   \n",
      "\n",
      "                                               content  \\\n",
      "238  Netflix has announced that David Letterman's n...   \n",
      "240  The Department of Justice has decided to revie...   \n",
      "241  Democratic National Committee Chairman Tom Per...   \n",
      "242  Senate Minority Leader Chuck Schumer, D-N.Y., ...   \n",
      "247  Former CIA Director Michael Hayden said Thursd...   \n",
      "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
      "249  Former U.S. President Bill Clinton on Monday c...   \n",
      "\n",
      "                                                 title         authors  \\\n",
      "238  Obama to Be First Guest on New David Letterman...             NaN   \n",
      "240  Daily Beast: DOJ Is Taking a Fresh Look Into H...    Mark Swanson   \n",
      "241  DNC's Tom Perez: Trump 'Cruel, Heartless' for ...  Jeffrey Rodack   \n",
      "242  Schumer: Trump Will Be Blamed for Any Govt Shu...  Jeffrey Rodack   \n",
      "247  Michael Hayden: We Should Be 'Frightened' by T...     Todd Beamon   \n",
      "248  Antonio Sabato Jr.: It's Oprah or Bust for Hol...   Bill Hoffmann   \n",
      "249  Bill Clinton Calls for Release of Reuters Jour...             NaN   \n",
      "\n",
      "                                         meta_keywords  \\\n",
      "238     ['david letterman', 'barack obama', 'netflix']   \n",
      "240  ['department of justice', 'daily beast', 'inve...   \n",
      "241  ['tom perez', 'slams', 'trump', 'cruel', 'salv...   \n",
      "242  ['chuck schumer', 'donald trump', 'blamed', 'g...   \n",
      "247  ['michael hayden', 'sthole countries', 'daca',...   \n",
      "248  ['antonio sabato jr', 'oprah winfrey', 'presid...   \n",
      "249  ['bill clinton', 'myanmar', 'calls', 'release'...   \n",
      "\n",
      "                                      meta_description  \\\n",
      "238  David Letterman Netflix Series Sets Barack Oba...   \n",
      "240  The Department of Justice has decided to revie...   \n",
      "241  Democratic National Committee Chairman Tom Per...   \n",
      "242  Senate Minority Leader Chuck Schumer, D-N.Y., ...   \n",
      "247  President Donald Trump's reported remarks abou...   \n",
      "248  Antonio Sabato Jr. says Hollywood's liberal el...   \n",
      "249  Former U.S. President Bill Clinton Calls for R...   \n",
      "\n",
      "                                                  tags  \n",
      "238  Trump Administration, ISIS/Islamic State, Gun ...  \n",
      "240  Trump Administration, ISIS/Islamic State, Gun ...  \n",
      "241  Donald Trump, Russia, Trump Administration, Gu...  \n",
      "242  Donald Trump, Russia, Trump Administration, Gu...  \n",
      "247  Homeland Security, Trump Administration, Immig...  \n",
      "248  Trump Administration, ISIS/Islamic State, News...  \n",
      "249  Donald Trump, Russia, Trump Administration, Gu...  \n"
     ]
    }
   ],
   "source": [
    "print(new_data.loc[new_data[\"domain\"] == \"www.newsmax.com\"]) # It seems odd that some domains have zero entries in 'type', so we print the entries associated with one of those domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f800a0",
   "metadata": {},
   "source": [
    "Vi undersøger, hvordan fordelingen er mellem artikeltyperne. Vi ser blandt andet at vores sample er domineret af 'fake' artikler, og at der ikke er nogle artikler af typen \"reliable\" og \"satire\", selvom der ifølge dokumentationen findes disse typer i det originale datasæt (https://github.com/several27/FakeNewsCorpus). Man kan derfor stille sig spørgsmålet, om samplen i det hele taget er repræsentativt for det originale datasæt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2395055",
   "metadata": {},
   "source": [
    "**Rensning af content variablen** <br>\n",
    "Der er potentiale for at rense content variablen, så man kan undersøge hyppigheden af specifikke ord. Det kræver dog, at man strukturerer dataen, således det er muligt at skelne mellem ord - også selvom de er adskilt af f.eks. \\n. Det giver først mening at lave denne rensning, hvis der er et mere specifikt mål eller behov med undersøgelsen.<br><br>\n",
    "For at se content, kan dette kode anvendes: *new_data['content'].value_counts()*\n",
    "\n",
    "**Rensning af tags variablen** <br>\n",
    "Vi så tidligere, at der kun er 24 unikke \"tag lister\". Med andre ord er der mange artikler, som ikke har tags tilknyttet. Det er brugbar viden, når man skal lave en model. Enten skal der implementeres noget kode, der sørger for at udfylde tags for de artikler, der mangler f.eks. ved hjælp af ovenstående content variable. Alternativt skal man ikke anvende \"tags\" variablen eller lave en entry 'ukendt'.\n",
    "<br><br>\n",
    "**Rensning af meta_keywords variablen** <br>\n",
    "Meta keywords er en kategori med lister af keywords. Her vil det være brugbart at rense for gentagne keywords i hhv. lower og uppercase samt fjerne tomme værdier i form af ''.\n",
    "\n",
    "For at se meta keywords, kan dette kode anvendes: *new_data['meta_keywords'].value_counts()*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b6cb2",
   "metadata": {},
   "source": [
    "I det følgende undersøger vi, om der er forfattere, som skriver for mere end et domæne. Det bemærkes, at \"Tom Rogan\" skriver for to forskellige, mens resten kun for en."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "400226a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "authors\n",
       "Tom Rogan                                                             2\n",
       "A Bad Witch'S Blog                                                    1\n",
       "Michael Rubin                                                         1\n",
       "Peter Schweizer                                                       1\n",
       "Pawan Kadu                                                            1\n",
       "                                                                     ..\n",
       "Fraser Institute, Because Without America, There Is No Free World.    1\n",
       "Firearms Radio Network                                                1\n",
       "Evan Wyatt                                                            1\n",
       "European Southern Observatory                                         1\n",
       "Zurich Times                                                          1\n",
       "Name: domain, Length: 109, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.groupby(['authors'])['domain'].nunique().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe57356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86        nationalreview.com\n",
      "89    washingtonexaminer.com\n",
      "Name: domain, dtype: object\n"
     ]
    }
   ],
   "source": [
    " print(new_data[\"domain\"].loc[new_data[\"authors\"] == \"Tom Rogan\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d2b51e",
   "metadata": {},
   "source": [
    "Herunder undersøger vi, hvor mange artikler, de forskellige forfattere har skrevet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4638bf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "John Rolls                                                        10\n",
       "Gerald Sinclair                                                    6\n",
       "The Daily Sheeple                                                  6\n",
       "Morgan Linton                                                      6\n",
       "Lisa Haven                                                         4\n",
       "                                                                  ..\n",
       "Pawan Kadu                                                         1\n",
       "Philip Wegmann                                                     1\n",
       "Kelly Cohen                                                        1\n",
       "John Anthony, Because Without America, There Is No Free World.     1\n",
       "Bill Hoffmann                                                      1\n",
       "Name: authors, Length: 109, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['authors'].value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af38620",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6991f3",
   "metadata": {},
   "source": [
    "**En 'type' (fake, hate etc) per domæne** <br>\n",
    "I forhold til observationerne vi har set, så er der kun en 'type' per domæne. Så det forventes, at domænet har meget at skulle sige omkring klassifikationen af artiklerne, hvis domænet er kendt i forvejen. \n",
    "Såfremt at en artikel kommer fra et ukendt domæne, så kan andre variabler hjælpe med at lave den rigtige klassifikation.  <br>\n",
    "\n",
    "**De fleste forfattere skriver kun for et domæne** <br>\n",
    "I vores sample size, skriver langt de fleste forfattere kun for et enkelt domæne, mens en enkelt forfatter skriver for to. Med andre ord, kan vi forvente en stærk korrelation mellem forfattere og domæner samt artiklernes 'type'. Dette kan potentielt bruges, hvis der er ukendte domæner, som en fremtidig model skal håndtere. Hvis det er af en kendt forfatter, kan vi udnytte denne korrelation. \n",
    "\n",
    "**Fake er overrepræsenteret i vores sample data** <br>\n",
    "Efter at kigget på det originale datasæt, kan det bl.a. konstateres, at 'typen' 'fake' er overrepræsenteret i vores sample i forhold til det originale datasæt. Endvidere ser vi antallet af 'typerne', der forekommer i det originale datasæt er forskellige, således de ikke kan vægtes og give et meningsfuldt gennemsnit. Det er relevant at overveje, hvis en model udvikles. <br>\n",
    "Med andre ord er 155 ud af 250 observationer i vores sample 'fake', hvilket vil sige, hvis vi tager en tilfældig observation, vi ikke ved noget om, vil det altid være et godt bud at vurdere, at den er 'fake', idet 'fake' udgør 62% af vores sample. I kontrast udgør 'fake' 9,86% af hele datasættet (https://github.com/several27/FakeNewsCorpus).<br>\n",
    "\n",
    "**Artikler mangler data eller mangler kategorisering** <br>\n",
    "Vi så i task 2, at der mangler data for nogle artikler. F.eks. manglede artikler fra www.newsmax.com en 'type' (om artiklen er 'fake', 'hate' etc) samt en forfatter. Dette er ikke et unikt tilfælde. Det har en påvirkning på en senere model, hvor man skal tage en beslutning om at inkludere disse data eller fjerne dem. Man skal dog være opmærksom på, at den manglende data også kan have information i sig selv. <br> Der er også artikler, som har 'typen' 'unknown', hvilket indeholder samme problemstilling som mangel på data.\n",
    "\n",
    "Første print nedenfor viser vi www.newsmax.com, der har None i 'type' og NaN i authors. Næste print er en illustration af data med tagget 'type', som er 'unknown'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "acb03451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                                                        249\n",
       "id                                                              39558\n",
       "domain                                                www.newsmax.com\n",
       "type                                                              NaN\n",
       "url                 https://www.newsmax.com/newsfront/bill-clinton...\n",
       "content             Former U.S. President Bill Clinton on Monday c...\n",
       "title               Bill Clinton Calls for Release of Reuters Jour...\n",
       "authors                                                           NaN\n",
       "meta_keywords       ['bill clinton', 'myanmar', 'calls', 'release'...\n",
       "meta_description    Former U.S. President Bill Clinton Calls for R...\n",
       "tags                Donald Trump, Russia, Trump Administration, Gu...\n",
       "Name: 249, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.loc[249]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84b742e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0     id                 domain     type  \\\n",
      "169         169  26554  undergroundhealth.com  unknown   \n",
      "178         178  27373  undergroundhealth.com  unknown   \n",
      "218         218  35900  strategic-culture.org  unknown   \n",
      "222         222  36471  strategic-culture.org  unknown   \n",
      "223         223  36475  strategic-culture.org  unknown   \n",
      "237         237  38330  strategic-culture.org  unknown   \n",
      "\n",
      "                                                   url  \\\n",
      "169  https://www.undergroundhealth.com/best-type-ma...   \n",
      "178  https://www.undergroundhealth.com/understandin...   \n",
      "218  https://www.strategic-culture.org/news/2017/01...   \n",
      "222  https://www.strategic-culture.org/pview/2017/1...   \n",
      "223  https://www.strategic-culture.org/news/2013/02...   \n",
      "237  https://www.strategic-culture.org/pview/2017/1...   \n",
      "\n",
      "                                               content  \\\n",
      "169  In our modern world, it is very common for peo...   \n",
      "178  by Giovani – Liveanddare.com\\n\\nAre you truly ...   \n",
      "218  Is Vietnam Tilting Toward China?\\n\\nXuan Loc D...   \n",
      "222  For the first time since the end of the Cold W...   \n",
      "223  The Return of Empires (II)\\n\\nPart I\\n\\nA Worl...   \n",
      "237  Trump's decision to sell advanced lethal weapo...   \n",
      "\n",
      "                                                 title authors  \\\n",
      "169          Best Type of Mattress for Herniated Discs     NaN   \n",
      "178          Understanding What Authentic Happiness Is     NaN   \n",
      "218                   Is Vietnam Tilting Toward China?     NaN   \n",
      "222  NATO Defense Chiefs and US Lawmakers Take New ...     NaN   \n",
      "223                         The Return of Empires (II)     NaN   \n",
      "237  Trump Now Arming Ukrainian Neo-Nazi Regime – E...     NaN   \n",
      "\n",
      "                                         meta_keywords  \\\n",
      "169                                               ['']   \n",
      "178                                               ['']   \n",
      "218                                               ['']   \n",
      "222  ['United States', 'Trump', 'INF Treaty', 'NDAA...   \n",
      "223                                               ['']   \n",
      "237                                               ['']   \n",
      "\n",
      "                                      meta_description  \\\n",
      "169  In our modern world, it is very common for peo...   \n",
      "178  Understanding what authentic happiness is. Is ...   \n",
      "218                                                NaN   \n",
      "222  The NATO defense chiefs and US lawmakers have ...   \n",
      "223                                                NaN   \n",
      "237  What, for instance, is the U.S. objective in U...   \n",
      "\n",
      "                                                  tags  \n",
      "169                                                NaN  \n",
      "178                                                NaN  \n",
      "218  Chatham House, Hague Tribunal, Sharp, Five Eye...  \n",
      "222                                                NaN  \n",
      "223  Chatham House, Hague Tribunal, Sharp, Five Eye...  \n",
      "237                                                NaN  \n"
     ]
    }
   ],
   "source": [
    "    print(new_data.loc[new_data[\"type\"] == \"unknown\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0096fb",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cffb34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find our letters\n",
    "group_nr = 2\n",
    "letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8418ccb",
   "metadata": {},
   "source": [
    "Efter at have inspiceret den givne hjemmeside, er det blevet tydeligt, at de urls, der forbindes med de enkelte bogstaver, ligner hinanden bortset fra sidste char, der afhænger af, hvilket bogstav, der er tale om.\n",
    "\n",
    "Derfor har vi valgt at lave en liste, der indeholder disse urls for herefter at kunne udforske hver enkelt side yderligere.\n",
    "\n",
    "Det har vi håndteret ved at anvende et for-loop, der itererer igennem en liste af de bogstaver, vi har fået givet ud fra vores gruppenr. Hvert bogstav placeres for enden af den fælles del af urlen, således der opstår en række unikke urls, der herefter tilføjes til listen url_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d2e8cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"https://en.wikinews.org\" # For later use\n",
    "\n",
    "# Create list with urls depending on letters\n",
    "url_list = []\n",
    "for l in letters:\n",
    "    url_list.append(f'https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from={l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99993a",
   "metadata": {},
   "source": [
    "Vi har implementeret et for-loop, der itererer igennem listen af urls (url_list) og listen af de bogstaver, vi har fået tildelt (letters) for at søge efter artikler, der starter med et givent bogstav.\n",
    "\n",
    "I første omgang stødte vi på den udfordring, at der i nogle tilfælde var en enkelt side med sådanne artikler, mens der i andre tilfælde var flere sider. Det løste vi ved at lave et while-loop, der sørger for at tjekke, om alle artikler med det pågældende forbogstav er fundet. Inde i while-loopet tjekker vi mere konkret, hvorvidt den sidste artikel på siden har det pågældende forbogstav. Hvis ikke, må det betyde, at der ikke er flere relevante artikler at finde. Men hvis det derimod er tilfældet, må det betyde, at der kan være en side med artikler mere. Derfor har vi lavet et stykke kode, der leder efter \"next page\"-knappen på siden og følger dennes link.\n",
    "\n",
    "Så længe while-loopet kører, bliver urlen læst. Der bliver oprettet en BeautifulSoup. Vi indsnævrer søgefeltet til den del af siden, vi er interesseret i vha. find og id, og vi finder selve artiklerne vha. find_all.\n",
    "\n",
    "Herfra har vi udtrukket titlerne på artiklerne ved brug af .text og gemt dem i en liste. Alle links har vi ligeledes gemt i en liste - udtrukket ved at anvende .get og søge efter 'href'. \n",
    "\n",
    "Vi har naturligvis kun været interesseret i at udtrække den relevante data, hvorfor vi har tilpasset koden flere steder f.eks. har vi tilføjet [:6] == \"/wiki/\" efter .get for at undgå links, der ikke linker til en artikel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7db3f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists\n",
    "all_titles = []\n",
    "all_links = []\n",
    "all_dates = []\n",
    "all_content = []\n",
    "all_sources = []\n",
    "\n",
    "# Loops through url list to retrieve titles and links\n",
    "for url, l in zip(url_list,letters):\n",
    "    \n",
    "    # Needed for doing multiple passes\n",
    "    all_found = False\n",
    "    page_url = url\n",
    "    \n",
    "    # While there are still some articles not found\n",
    "    while not all_found:\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        container = soup.find(id = \"mw-pages\")\n",
    "        data = container.find_all('li')\n",
    "        \n",
    "        # Added so it only takes titles with the correct first letter\n",
    "        titles = [m.text for m in data if m.text[0].upper()==l]\n",
    "        \n",
    "        # Append the titles to the running list\n",
    "        all_titles += titles\n",
    "        \n",
    "        # Add links to our list\n",
    "        links = [base + link.get('href') for link in container.find_all('a') if link.get('href')[:6] == \"/wiki/\"]\n",
    "        \n",
    "        all_links += links[:len(titles)]\n",
    "        \n",
    "        # If the last element on the page is not part of our letter we go to the next\n",
    "        if data[-1].text[0] != l:\n",
    "            break\n",
    "            \n",
    "        # Otherwise we find the \"next page\" button and follows it\n",
    "        buttons = soup.findAll('a', href=True)\n",
    "        for b in buttons:\n",
    "            try:\n",
    "                if b.contents[0] == \"next page\":\n",
    "                    page_url = base + b[\"href\"]\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fcdc68",
   "metadata": {},
   "source": [
    "Med en liste af artiklernes links har det derfra været muligt for os at lave en for-loop, der itererer igennem alle artikler, læser urlen, opretter en BeautifulSoup, indsnævrer søgeområdet på siden og herfra udtrækker den data, vi er interesseret i.\n",
    "\n",
    "Ved anvendelse af .text og find_all har vi udtrukket alle datoer og tilføjet dem til en liste. Vi har udtrukket artiklernes indhold samt referencekilder og ligeledes tilføjet dem til lister.\n",
    "\n",
    "De endelige lister med data, vi ender ud med er all_titles, all_links, all_dates, all_content samt all_sources.\n",
    "\n",
    "Som tidligere nævnt kan det være tidskrævende at loope igennem ca. 2650 artikler, hvorfor vi har gjort brug af *tqdm*, så det er muligt at følge, hvor mange artikler, den har indlæst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3b248f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61627b84808245c394c9e2b9aa87841b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loops through links to retrieve data from each articles\n",
    "for article in tqdm(all_links):\n",
    "    response = requests.get(article)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    container = soup.find('div', class_ = \"mw-parser-output\")\n",
    "\n",
    "    # Date\n",
    "    date = [d.text for d in container.find_all(\"strong\", class_ = \"published\")]\n",
    "    all_dates.append(date)\n",
    "\n",
    "    # Content\n",
    "    content = [c.text for c in container.find_all(\"p\")[1:]]\n",
    "    all_content.append(content)\n",
    "\n",
    "    # Sources\n",
    "    sources = [s.text for s in container.find_all(\"span\", class_ = \"sourceTemplate\")]\n",
    "    all_sources.append(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200bb148",
   "metadata": {},
   "source": [
    "Vi har organiseret vores data ved at navngive kolonner og specificere, hvilken liste hver kolonne skal hente data fra. Herefter har vi benyttet *pandas* til at lave en dataframe ud fra vores data. Til sidst eksporterer vi denne dataframe til hhv. csv og excel.\n",
    "\n",
    "Det færdige datasæt indeholder 2647 rækker (scrapet 3. marts) samt 5 kolonner. Kolonnerne omfatter titler på hver enkelt artikel, deres urls/links, dato for, hvornår hver enkelt artikel er skrevet, de enkelte artiklers indhold dvs. selve artikelteksten samt den eller de kilder, den enkelte artikel er udsprunget af."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f73d3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing data\n",
    "data = {'Title': all_titles, 'Link': all_links, 'Date': all_dates, 'Content': all_content, 'Sources': all_sources}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3141cdc",
   "metadata": {},
   "source": [
    "I forbindelse med export af data til excel, skal man være opmærksom på, at det kan være nødvendigt at installere pakken *openpyxl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3a52f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from data\n",
    "df_politics_conflict = pd.DataFrame(data)\n",
    "\n",
    "# Export data as csv and excel\n",
    "df_politics_conflict.to_csv(\"wikinewsdata.csv\")\n",
    "df_politics_conflict.to_excel(\"wikinewsdata.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de6378",
   "metadata": {},
   "source": [
    "**No fake/no-fake label**\n",
    "<br>\n",
    "Efter at have arbejdet i dybden med websitet har vi vurderet, at det ikke er et fornuftigt valg at anskue artiklerne som troværdige.\n",
    "\n",
    "Vi scraper fra et website, hvor enhver kan oprette sig som bruger og redigere indholdet. Vi har selv testet muligheden for at redigere og oprette artikler. Det kræver ikke engang en e-mail at få lov at lægge nyt indhold op eller redigere allerede eksisterende indhold. Det vil sige som enkelt person kan man nemt oprette flere tusinde konti på kort tid. Det eneste sikkerhedstjek, man møder, er captcha.\n",
    "\n",
    "Dertil kan redigering forekomme på alle tidspunker, hvilket vil sige, at vores datasæt kan være ændret for hver gang vi scraper.\n",
    "\n",
    "Det betyder i bund og grund, at der nemt kan opstå fake news artikler. Der vil naturligvis være en risiko forbundet med at stole på brugergenerede artikler - som er tilfældet her. Derfor vil man skulle være yderst påpasseligt med at anvende data fra denne type sider."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
