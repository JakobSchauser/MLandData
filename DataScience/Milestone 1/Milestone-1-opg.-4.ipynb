{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b0a173b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff <3\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43d4c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find our letters\n",
    "group_nr = 2\n",
    "letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75c64740",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"https://en.wikinews.org/\" # For later use\n",
    "\n",
    "# Create list with urls depending on letters\n",
    "url_list = []\n",
    "for l in letters:\n",
    "    url_list.append('https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from={}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d683908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists\n",
    "all_titles = []\n",
    "all_links = []\n",
    "all_dates = []\n",
    "all_content = []\n",
    "all_sources = []\n",
    "\n",
    "# Loops through url list to retrieve titles and links\n",
    "for url,l in zip(url_list,letters):\n",
    "    \n",
    "    # Needed for doing multiple passes\n",
    "    all_found = False\n",
    "    page_url = url\n",
    "    \n",
    "    # While there are still some articles not found\n",
    "    while not all_found:\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        container = soup.find(id = \"mw-pages\")\n",
    "        \n",
    "        data = container.find_all('li')\n",
    "        \n",
    "        # Add links to our list\n",
    "        links = [base + link.get('href') for link in container.find_all('a') if link.get('href')[:6] == \"/wiki/\"]\n",
    "        \n",
    "        all_links += links\n",
    "        \n",
    "        # Added so it only takes titles with the correct first letter\n",
    "        titles = [m.text for m in data if m.text[0].upper()==l]\n",
    "        \n",
    "        # Append the titles to the running list\n",
    "        all_titles += titles\n",
    "        \n",
    "        # If the last element on the page is not part of our letter we go to the next\n",
    "        if data[-1].text[0] != l:\n",
    "            break\n",
    "            \n",
    "        # Otherwise we find the \"next page\" button and follows it\n",
    "        buttons = soup.findAll('a', href=True)\n",
    "        for b in buttons:\n",
    "            try:\n",
    "                if b.contents[0] == \"next page\":\n",
    "                    page_url = base + b[\"href\"]\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4e17615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops through links to retrieve data from each articles\n",
    "for article in all_links[0:9]:\n",
    "    response = requests.get(article)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    container = soup.find('div', class_ = \"mw-parser-output\")\n",
    "\n",
    "    # Date\n",
    "    date = [d.text for d in container.find_all(\"strong\", class_ = \"published\")]\n",
    "    all_dates += date\n",
    "\n",
    "    # Content\n",
    "    content = [c.text for c in container.find_all(\"p\")[1:]]\n",
    "    all_content += content\n",
    "\n",
    "    # Sources\n",
    "    sources = [s.text for s in container.find_all(\"span\", class_ = \"sourceTemplate\")]\n",
    "    all_sources += sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d498f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing data\n",
    "data = {'Title': all_titles, 'Link': all_links, 'Date': all_dates, 'Content': all_content, 'Sources': all_sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "caa30ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export data as csv and excel\n",
    "df.to_csv(\"/Users/gabriellemadsen/Library/CloudStorage/OneDrive-Personligt/Uddannelse/Machine Learning & Datavidenskab/2. semester/DS/Milestone_1/wikinewsdata.csv\")\n",
    "df.to_excel(\"/Users/gabriellemadsen/Library/CloudStorage/OneDrive-Personligt/Uddannelse/Machine Learning & Datavidenskab/2. semester/DS/Milestone_1/wikinewsdata.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b0e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
