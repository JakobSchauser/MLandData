{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d41f1bbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T15:41:11.867544Z",
     "start_time": "2022-02-19T15:40:58.489095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import stuff <3\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Find our letters\n",
    "group_nr = 2\n",
    "letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "\n",
    "base =  \"https://en.wikinews.org/\"\n",
    "\n",
    "# List with URLs depend on letters\n",
    "url_list = []\n",
    "for l in letters:\n",
    "    url_list.append('https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from={}'.format(l))\n",
    "    \n",
    "# Loop through url list and print all titles\n",
    "for url,l in zip(url_list,letters):\n",
    "    \n",
    "    # Needed for doing multiple passes\n",
    "    all_found = False\n",
    "    page_url = url\n",
    "    all_titles = []\n",
    "    \n",
    "    # While there are still some articles not found\n",
    "    while not all_found:\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        container = soup.find(id = \"mw-pages\")\n",
    "        \n",
    "        data = container.find_all('li')\n",
    "        \n",
    "        # Added so it only takes titles with the correct first letter\n",
    "        titles = [m.text for m in data if m.text[0].upper()==l]\n",
    "        \n",
    "        # Append the titles to the running list\n",
    "        all_titles += titles\n",
    "        \n",
    "        # If the last element on the page is not part of our letter we go to the next\n",
    "        if data[-1].text[0] != l:\n",
    "            break\n",
    "            \n",
    "        # Otherwise we find the \"next page\" button and follows it\n",
    "        buttons = soup.findAll('a', href=True)\n",
    "        for b in buttons:\n",
    "            try:\n",
    "                if b.contents[0] == \"next page\":\n",
    "                    page_url = base + b[\"href\"]\n",
    "            except:\n",
    "                pass\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca76f217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-19T15:35:19.037439Z",
     "start_time": "2022-02-19T15:35:19.030459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
