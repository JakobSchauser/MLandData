{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9a4cb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-02-21T14:47:31.524Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import stuff <3\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Find our letters\n",
    "group_nr = 2\n",
    "letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]\n",
    "\n",
    "base =  \"https://en.wikinews.org/\"\n",
    "\n",
    "# List with URLs depend on letters\n",
    "url_list = []\n",
    "for l in letters:\n",
    "    url_list.append('https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from={}'.format(l))\n",
    "    \n",
    "# Loop through url list and print all titles\n",
    "for url,l in zip(url_list,letters):\n",
    "    \n",
    "    # Needed for doing multiple passes\n",
    "    all_found = False\n",
    "    page_url = url\n",
    "    all_titles = []\n",
    "    all_links = []\n",
    "    all_dates = []\n",
    "    all_content = []\n",
    "    all_sources = []\n",
    "    \n",
    "    # While there are still some articles not found\n",
    "    while not all_found:\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        container = soup.find(id = \"mw-pages\")\n",
    "        \n",
    "        data = container.find_all('li')\n",
    "        \n",
    "        # Add links to list\n",
    "        # TO DO:\n",
    "        # LIGE NU INDSÆTTES DER OGSÅ LINKS I LISTEN, SOM IKKE BØR VÆRE DER:\n",
    "            # NEMLIG NEXTPAGE LINKSENE - VI SKAL KUN BRUGE ARTIKEL LINKS\n",
    "        # DESUDEN KAN VI KUN KOMME IND PÅ LINKSENE, HVIS DE STARTER MED https://en.wikinews.org, HVILKET DE IKKE GØR PT\n",
    "        links = [link.get('href') for link in container.find_all('a') if link.get('href')[:6] == \"/wiki/\"]\n",
    "\n",
    "        all_links += links\n",
    "        \n",
    "        # Added so it only takes titles with the correct first letter\n",
    "        titles = [m.text for m in data if m.text[0].upper()==l]\n",
    "        \n",
    "        # Append the titles to the running list\n",
    "        all_titles += titles\n",
    "        \n",
    "        # If the last element on the page is not part of our letter we go to the next\n",
    "        if data[-1].text[0] != l:\n",
    "            break\n",
    "            \n",
    "        # Otherwise we find the \"next page\" button and follows it\n",
    "        buttons = soup.findAll('a', href=True)\n",
    "        for b in buttons:\n",
    "            try:\n",
    "                if b.contents[0] == \"next page\":\n",
    "                    page_url = base + b[\"href\"]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    \n",
    "# Loop through each links to retrive date from the articles\n",
    "# TO DO:\n",
    "# HERUNDER SKAL all_links listen INDSÆTTES I STEDET FOR NEDENSTÅENDE LINKS\n",
    "# KODEN VIRKER FINT, SÅ LÆNGE DEN FÅR GYLDIGE LINKS\n",
    "# SKULLE VÆRE I ORDEN, NÅR all_links ER I ORDEN\n",
    "for article in [\"https://en.wikinews.org/wiki/Carter:_Race_relations_in_Palestine_are_worse_than_apartheid\"]: \n",
    "    response = requests.get(article)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    container = soup.find('div', class_ = \"mw-parser-output\")\n",
    "\n",
    "    # Date\n",
    "    date = [d.text for d in container.find_all(\"strong\", class_ = \"published\")]\n",
    "    all_dates += date\n",
    "\n",
    "    # Content\n",
    "    # TO DO: DATE SKAL IKKE MED I LISTEN OVER CONTENT\n",
    "    content = [c.text for c in container.find_all(\"p\")]\n",
    "    all_content += content\n",
    "\n",
    "    # Sources\n",
    "    sources = [s.text for s in container.find_all(\"span\", class_ = \"sourceTemplate\")]\n",
    "    all_sources += sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493561c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
