{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ebc403",
   "metadata": {},
   "source": [
    "Milestone 1:\n",
    "\n",
    "by \n",
    "Roman Sekandari | czw500\n",
    "Josef Blom      | pns188\n",
    "Rasmus Jensen   | lxp826"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c756b8",
   "metadata": {},
   "source": [
    "Task 2\n",
    "\n",
    "First off we'll import our packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a996a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/RomanSekandari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c8886",
   "metadata": {},
   "source": [
    "The package re is used to filter the data we pulled, to remove data we don't want like capital letters, dots, commas and numbers.\n",
    "\n",
    "The nltk packages is used to stem our data, and making the data as an array with stopwords.\n",
    "We used it's built in stopwords package because it was already integrated.\n",
    "\n",
    "The request package is used to get data from a website. We used because we felt it was more convinient to let the program get the data from the website itself rather than having us download the file.\n",
    "\n",
    "\n",
    "The package bs4 has the extention BeutifulSoup which we used to read and pull a websites HTML-code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e10b1d4",
   "metadata": {},
   "source": [
    "We then pull the websites data as a string and print the linkContents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d257c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getLink = requests.get('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "linkContents = getLink.text\n",
    "linkParsing = BeautifulSoup(linkContents, 'html.parser')\n",
    "\n",
    "#print(linkContents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0282832",
   "metadata": {},
   "source": [
    "We then have a string called linkContents, which is the data from the website as a string. We will then clean the data by changing all capital letters to lower case letters from a-z with only spaces, with no commas, dots or numbers\n",
    "\n",
    "We then name the new string 'data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbece55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = linkContents.lower()\n",
    "data = re.sub('[^A-Za-z]+', ' ', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf508d61",
   "metadata": {},
   "source": [
    "We'll then clean up the data even more by removing stopwords from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e413ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "data = data.split()\n",
    "none =[]\n",
    "for i in range(len(data)):\n",
    "    if data[i] not in stopwords:\n",
    "        none.append(data[i])\n",
    "data = ' '.join(map(str, none))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a87709",
   "metadata": {},
   "source": [
    "With the cleaned up data we then shorten the words to their roots, eg. playing -> play, by looking in the string array which is created and taking 1 element at the time. If a word is already at it's root state, it will be shortened even more which will make the word \"weird\", eg 'believe' -> 'believe'\n",
    "\n",
    "stemmingData will then be a string array with only words, which is shortened. This also means that stuff like a domain ('awm.com') will be shortened to 2 seperate string 'awm' and 'com'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7ca2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = PorterStemmer()\n",
    "stemmingData=[]\n",
    "for data in none:\n",
    "    stemmingData.append(stem.stem(data))\n",
    "    \n",
    "#print(stemmingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11190fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n"
     ]
    }
   ],
   "source": [
    "print(stemmingData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac405d2d",
   "metadata": {},
   "source": [
    "Task 3:\n",
    "\n",
    "Looking at the contents of the original data set, we see a particularly biased point-of-view. This point-of-view has an effect on our data, in the sense that the words being used are in a context of a specific viewpoint. If the data were to be as clean as possible, unbiased data would have been a better solution if we were trying to make something for regular natural language. But because our focus is to use these data as a Fake News Detector, it is relevant to our case. \n",
    "Looking at the origin of these articles, it is clear that the objectivity and facts are unclear. The basis of the articles aren’t consistent in their gathering of objective data or trusted sources.\n",
    "\n",
    "We have assumption that the similarity throughout many of the articles; They have a tendency to be unreliable and subjective. Many of which is about guns, christianity etc. In other words, the variance in the material that is processed in our algorithm, is not wide enough or objective enough.\n",
    "\n",
    "In some of the columns, there is missing data. This can have the effect of inconsistency in our dataset. Thus, it is important that we have cleaned the data, but it will still have an impact on the final outcome of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f6bb4",
   "metadata": {},
   "source": [
    "Task 4: \n",
    "\n",
    "We are making an algorithm which gets the headline and publish date from a website. We chose a wiki site and then got the headline and the publish date of the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54993c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: ['Russia continues to attack Ukraine amid sanctions from the west']\n"
     ]
    }
   ],
   "source": [
    "#userInputLink=input(\"Input URL from wikipedia: \")\n",
    "\n",
    "userInputLink = \"https://en.wikinews.org/wiki/Russia_continues_to_attack_Ukraine_amid_sanctions_from_the_west\"\n",
    "\n",
    "getUserInputLink = requests.get(userInputLink)\n",
    "userlinkContents = getUserInputLink.text\n",
    "userLinkParsing = BeautifulSoup(userlinkContents, 'html.parser')\n",
    "\n",
    "userLinkHeadline = [link.string for link in userLinkParsing.find_all('h1')] \n",
    "print(\"Headline:\", userLinkHeadline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e40a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish date: Monday, February 28, 2022 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = mydivs = userLinkParsing.find_all(id='publishDate')\n",
    "date = ' '.join(map(str, date))\n",
    "date = re.sub('[^0-9]+', ' ', date)\n",
    "\n",
    "\n",
    "print(\"Publish date:\", userLinkParsing.find('p').text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66141c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
