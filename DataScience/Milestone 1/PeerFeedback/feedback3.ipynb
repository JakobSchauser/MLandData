{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data as a DataFrame with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('news_sample.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a custom function for cleaning the text. This is due to the fact that the clean-text library does not identify URL's that omits www. We also need to find emails before URL's, so we include our own regular expression. We use simple date formats only. We use the clean function from clean-text to clean up the rest, which is line breaks, punctuations, numbers, excess white spaces and making the text lowercase. We apply the function to each content section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_help(text: str) -> str:\n",
    "    email = r\"[a-zA-Z0-9.%+-]+@[a-zA-Z0-9.%+-]+\\.[a-z]{2,4}\"\n",
    "    date = r\"[0-9]{1,2}[/-][0-9]{1,2}[/-][0-9]{1,4}\"\n",
    "    url = r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\"\n",
    "\n",
    "    text = re.sub(email, \"<EMAIL>\", text)\n",
    "    text = re.sub(url, \"<URL>\", text)\n",
    "    text = re.sub(date, \"<DATE>\", text)\n",
    "    return clean(text,\n",
    "                 lower=True,\n",
    "                 no_line_breaks=True,\n",
    "                 no_numbers=True,\n",
    "                 no_punct=True)\n",
    "\n",
    "contents = df['content'].apply(lambda x: clean_text_help(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used RegexpTokenizer to create a tokenizer that would treat tags such as \\<URL\\> as one token instead of three tokens being \"<\", \"URL\" and \">\". RegexpTokenizer is from the nltk.tokenizer library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'<?\\w+>?')\n",
    "tokens = contents.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We remove stopwords from each list of tokens, corresponding to the different content sections. We use a list of stopwords from the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stopwords = []\n",
    "for token in tokens:\n",
    "    no_stopwords.append([word for word in token if word not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stem each list of text using the PorterStemmer from the nltk library. This reduces the vocabulary substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed = []\n",
    "for lst in no_stopwords:\n",
    "    stemmed.append([ps.stem(w) for w in lst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset of articles all the fake news were from the domain beforeitsnews.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of fake news:  beforeitsnews.com    155\n",
      "Name: domain, dtype: int64\n",
      "Articles in beforeitsnews.com    155\n",
      "Name: domain, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "fake = []\n",
    "for idx, val in enumerate(df[\"type\"]):\n",
    "    if val == \"fake\":\n",
    "        fake.append(idx)\n",
    "print(\"Distribution of fake news: \", df[\"domain\"][fake].value_counts())\n",
    "\n",
    "number = []\n",
    "for idx, val in enumerate(df[\"domain\"]):\n",
    "    if val == \"beforeitsnews.com\":\n",
    "        number.append(idx)\n",
    "print(\"Articles in\", df[\"domain\"][number].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the content contains the word trump, there is an 82,5 % chance that it is fake news. Knowing that all fake news come from beforeitnews.com and all news from beforeitnews.com are fake, we see that 132 articles with the word \"trump\" in it are fake and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chance of fake news if the word \"trump\" is in the text:  0.825\n",
      "Sources of articles with the word \"trump\" in it:\n",
      "beforeitsnews.com           132\n",
      "washingtonexaminer.com        8\n",
      "canadafreepress.com           6\n",
      "www.newsmax.com               5\n",
      "strategic-culture.org         2\n",
      "bipartisanreport.com          1\n",
      "charismanews.com              1\n",
      "willyloman.wordpress.com      1\n",
      "21stcenturywire.com           1\n",
      "alternet.org                  1\n",
      "vdare.com                     1\n",
      "americanlookout.com           1\n",
      "Name: domain, dtype: int64\n",
      "Articles mentioning trump:  160\n"
     ]
    }
   ],
   "source": [
    "trump_indexes = []\n",
    "\n",
    "for idx, lst in enumerate(stemmed):\n",
    "    if \"trump\" in lst:\n",
    "        trump_indexes.append(idx)\n",
    "print(\"The chance of fake news if the word \\\"trump\\\" is in the text: \",\n",
    "      (df['type'][trump_indexes] == \"fake\").sum() / len(trump_indexes))\n",
    "print(\"Sources of articles with the word \\\"trump\\\" in it:\")\n",
    "print(df[\"domain\"][trump_indexes].value_counts())\n",
    "print('Articles mentioning trump: ', len(trump_indexes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that if the content contains the word bitcoin, then there is a 100 % chance that it is fake news. Again, all the fake news are from the domain beforeitsnews.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chance of fake news if the word \"bitcoin\" is in the text:  1.0\n",
      "Amount of articles mentioning bitcoins:  131\n"
     ]
    }
   ],
   "source": [
    "bitcoin_index = []\n",
    "for idx, lst in enumerate(stemmed):\n",
    "    if \"bitcoin\" in lst:\n",
    "        bitcoin_index.append(idx)\n",
    "print(\"The chance of fake news if the word \\\"bitcoin\\\" is in the text: \",\n",
    "      (df[\"type\"][bitcoin_index] == \"fake\").sum()/len(bitcoin_index))\n",
    "print('Amount of articles mentioning bitcoins: ', len(bitcoin_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both words \"trump\" and \"bitcoin\" are in an article the article is produced by beforeitnews.com and based on the previous tests we know it is fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source and count of articles containing the word \"trump\" and \"bitcoin\": beforeitsnews.com    130\n",
      "Name: domain, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "TrumpNBitcoin_index = []\n",
    "\n",
    "for idx, lst in enumerate(stemmed):\n",
    "    if \"trump\" in lst and \"bitcoin\" in lst:\n",
    "        TrumpNBitcoin_index.append(idx)\n",
    "print(\"Source and count of articles containing the word \\\"trump\\\" and \\\"bitcoin\\\":\",\n",
    "      df[\"domain\"][TrumpNBitcoin_index].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 85% of fake news is about either Trump or bitcoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8580645161290322\n"
     ]
    }
   ],
   "source": [
    "total_fake_Trump_bitcoin = (len(bitcoin_index) - len(TrumpNBitcoin_index)) + (\n",
    "    (df['type'][trump_indexes] == \"fake\").sum() - len(TrumpNBitcoin_index)) + len(TrumpNBitcoin_index)\n",
    "print(total_fake_Trump_bitcoin / len(number))\n",
    "\n",
    "print(bitcoin_index)\n",
    "print(TrumpNBitcoin_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we use BeautifulSoup to scrape news articles. We use json to store the data in a .json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the letters for our group, and the main link. Get the content of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[16%23:16%23+10]\n",
    "main_link = \"https://en.wikinews.org/wiki/Category:Politics_and_conflicts\"\n",
    "main = requests.get(main_link)\n",
    "content = main.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the link to the section where articles starting with 'A' is listed. The idea is to get all article links from this page, then use the 'next page' link to also get articles from the next page. This way, we avoid skipping articles as we would if we used the subsection for 'B' and 'C' and so on, since they only display 200 articles in a page. So 'next page' is the way to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(content, 'html.parser')\n",
    "category = [obj['href'] for obj in soup1.find_all(\n",
    "    'a',  {\"class\": \"external text\"}, href=True) if obj.get_text() in 'A']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there is a 'next page' button, get the page and extract all the articles that are in our letter group. We do this by finding the 'div' tag that encapsulates the section of the web page that we want to look at, then finding all the 'a' tags, because they contain links. We then extract the 'href' from these. The 'next page' object will always be one of the 2 first links in this list. We then put the resulting links into a list if they are not the 'next page' or 'previous page' links, and if their first letter is from our letter group. We end up with a list of article links containing all the articles we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "next_page = category\n",
    "while next_page:\n",
    "    page = requests.get(next_page[0] if 'http' in next_page[0]\n",
    "                        else 'https://en.wikinews.org' + next_page[0]).text\n",
    "\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    parent = soup.find('div', {'id': 'mw-pages'})\n",
    "    article_objects = parent.find_all('a', href=True)\n",
    "\n",
    "    next_page = [obj['href'] for obj in article_objects[0:2] if\n",
    "                 obj.get_text() == 'next page'\n",
    "                 ]\n",
    "\n",
    "    article_links = [obj['href'] for obj in article_objects if\n",
    "                     (obj.get_text() != 'next page')        and\n",
    "                     (obj.get_text() != 'previous page')    and\n",
    "                     (obj.get_text()[0] in letters)\n",
    "                     ]\n",
    "\n",
    "    links.extend(article_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to extract some metadata from all the articles. We want the headlines, dates and contents. So for each link, the headline is the same 'h1' in all articles. The content is all the 'p', 'h2' and 'li' tags, and the dates are usually a 'strong' tag. The date is the most complicated one, since it is not consistently done the same way. We check if we get a date from the 'strong' tag, and if we don't it is usually in a 'b' tag. We find this 'b' tag, then append it to the dates list only if we find it, and it has the right format, which we check with regular expression search. If the conditions don't apply, we give a None value to the list instead, as we cannot find the correct date (some articles have no dates). The date has been a particular challenge, but we are convinced (without checking through the whole data) that we solved it correctly most, if not all, of the time. This cell will take multiple minutes to run. It took over 6 minutes for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "dates = []\n",
    "contents = []\n",
    "\n",
    "for link in links:\n",
    "    page = requests.get('https://en.wikinews.org' + link).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    headline = soup.find('h1', {\"id\": \"firstHeading\"}).get_text()\n",
    "    headlines.append(headline)\n",
    "\n",
    "    parent = soup.find('div', {\"class\": \"mw-parser-output\"})\n",
    "    date = parent.find('strong', {'class': 'published'})\n",
    "    content = [obj.get_text().rstrip('\\n')\n",
    "               for obj in parent.find_all(['p', 'h2', 'li'])]\n",
    "\n",
    "    if date:\n",
    "        dates.append(date.get_text())\n",
    "    else:\n",
    "        date = parent.find('b')\n",
    "        dates.append(date.get_text() if date and re.search(\n",
    "            r'\\w*[0-9]+', date.get_text()) else None)\n",
    "\n",
    "    contents.append('\\n'.join(content[1:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the data together and export it to a json file. This file can be importet into new scripts when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to json\n",
    "data = [{'Headline': x, 'Date': y, 'Content': z}\n",
    "        for x, y, z in zip(headlines, dates, contents)]\n",
    "\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(data, fp, indent=2, ensure_ascii=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
