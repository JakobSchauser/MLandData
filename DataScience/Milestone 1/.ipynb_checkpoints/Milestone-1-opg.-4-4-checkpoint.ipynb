{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6d7b2c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:24:04.425254Z",
     "start_time": "2022-02-24T14:24:04.420269Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import stuff <3\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb94b6d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:24:04.650034Z",
     "start_time": "2022-02-24T14:24:04.645050Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find our letters\n",
    "group_nr = 2\n",
    "letters = \"ABCDEFGHIJKLMNOPRSTUVWZABCDEFGHIJKLMNOPRSTUVWZ\"[group_nr%23:group_nr%23+10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1305d",
   "metadata": {},
   "source": [
    "Efter at have inspiceret den givne hjemmeside, er det blevet tydeligt, at de urls, der forbindes med de enkelte bogstaver, ligner hinanden bortset fra sidste karakter, der afhænger af, hvilket bogstav, der er tale om.\n",
    "\n",
    "Derfor har vi valgt at lave en liste, der indeholder disse urls for herefter at kunne udforske hver enkelt side yderligere.\n",
    "\n",
    "Det har vi håndteret ved at anvende et for-loop, der itererer igennem en liste af de bogstaver, vi har fået givet ud fra vores gruppenr. Hver bogstav placeres for enden af den fælles del af urlen, således der opstår en række unikke urls, der herefter tilføjes til listen url_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc2ad53b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:29:07.525036Z",
     "start_time": "2022-02-24T14:29:07.521049Z"
    }
   },
   "outputs": [],
   "source": [
    "base = \"https://en.wikinews.org\" # For later use\n",
    "\n",
    "# Create list with urls depending on letters\n",
    "url_list = []\n",
    "for l in letters:\n",
    "    url_list.append(f'https://en.wikinews.org/w/index.php?title=Category:Politics_and_conflicts&from={l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30014f3a",
   "metadata": {},
   "source": [
    "Vi har implementeret et for-loop, der itererer igennem listen af urls (url_list) og listen af de bogstaver, vi har fået tildelt (letters) for at søge efter artikler, der starter med et givent bogstav.\n",
    "\n",
    "I første omgang stødte vi på den udfordring, at der i nogle tilfælde var en enkelt side med sådanne artikler, mens der i andre tilfælde var flere sider. Det løste vi ved at lave et while-loop, der sørger for at tjekke, om alle artikler med det pågældende forbogstav er fundet. Inde i while-loopet tjekker vi mere konkret, hvorvidt den sidste artikel på siden har det pågældende forbogstav. Hvis ikke, må det betyde, at der ikke er flere relevante artikler at finde. Men hvis det derimod er tilfældet, må det betyde, at der kan være en side med artikler mere. Derfor har vi lavet et stykke kode, der leder efter \"next page\"-knappen på siden og følger dennes link.\n",
    "\n",
    "Så længe while-loopet kører, bliver urlen læst. Der bliver oprettet en BeautifulSoup. Vi indsnævrer søgefeltet til den del af siden, vi er interesseret i vha. find og id. Og vi finder selve artiklerne vha. find_all.\n",
    "\n",
    "Herfra har vi udtrukket titlerne på artiklerne ved brug af .text og gemt dem i en liste. Alle links har vi ligeledes gemt i en liste - udtrukket ved at anvende .get og søge efter 'href'. \n",
    "\n",
    "Vi har naturligvis kun været interesseret i at udtrække den relevante data, hvorfor vi har tilpasses koden flere steder f.eks. har vi tilføjet [:6] == \"/wiki/\" efter .get for at undgå links, der ikke linker til en artikel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d404bff0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:29:22.760441Z",
     "start_time": "2022-02-24T14:29:08.104612Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create empty lists\n",
    "all_titles = []\n",
    "all_links = []\n",
    "all_dates = []\n",
    "all_content = []\n",
    "all_sources = []\n",
    "\n",
    "# Loops through url list to retrieve titles and links\n",
    "for url, l in zip(url_list,letters):\n",
    "    \n",
    "    # Needed for doing multiple passes\n",
    "    all_found = False\n",
    "    page_url = url\n",
    "    \n",
    "    # While there are still some articles not found\n",
    "    while not all_found:\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        container = soup.find(id = \"mw-pages\")\n",
    "        data = container.find_all('li')\n",
    "        \n",
    "        # Added so it only takes titles with the correct first letter\n",
    "        titles = [m.text for m in data if m.text[0].upper()==l]\n",
    "        \n",
    "        # Append the titles to the running list\n",
    "        all_titles += titles\n",
    "        \n",
    "        # Add links to our list\n",
    "        links = [base + link.get('href') for link in container.find_all('a') if link.get('href')[:6] == \"/wiki/\"]\n",
    "        \n",
    "        all_links += links[:len(titles)]\n",
    "        \n",
    "        # If the last element on the page is not part of our letter we go to the next\n",
    "        if data[-1].text[0] != l:\n",
    "            break\n",
    "            \n",
    "        # Otherwise we find the \"next page\" button and follows it\n",
    "        buttons = soup.findAll('a', href=True)\n",
    "        for b in buttons:\n",
    "            try:\n",
    "                if b.contents[0] == \"next page\":\n",
    "                    page_url = base + b[\"href\"]\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a697c4",
   "metadata": {},
   "source": [
    "Med en liste af artiklernes links har det derfra været muligt for os at lave en for-loop, der itererer igennem alle artikler, læser urlen, opretter en BeautifulSoup, indsnævrer søgeområdet på siden og herfra udtrækker den data, vi er interesseret i.\n",
    "\n",
    "Ved anvendelse af .text og find_all har vi udtrukket alle datoer og tilføjet dem til en liste. Vi har udtrukket artiklernes indhold samt referencekilder og ligeledes tilføjet til lister.\n",
    "\n",
    "De endelige lister med data, vi ender ud med er all_titles, all_links, all_dates, all_content samt all_sources.\n",
    "\n",
    "Da det kan være tidskrævende at loope igennem ca. 2650 artikler, har vi gjort brug af tqdm, så det er muligt at følge, hvor mange artikler, den har indlæst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5faf06ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:38:00.786821Z",
     "start_time": "2022-02-24T14:29:22.762418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea2da9f930f44448cc5da492343f6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loops through links to retrieve data from each articles\n",
    "for article in tqdm(all_links):\n",
    "    response = requests.get(article)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    container = soup.find('div', class_ = \"mw-parser-output\")\n",
    "\n",
    "    # Date\n",
    "    date = [d.text for d in container.find_all(\"strong\", class_ = \"published\")]\n",
    "    all_dates.append(date)\n",
    "\n",
    "    # Content\n",
    "    content = [c.text for c in container.find_all(\"p\")[1:]]\n",
    "    all_content.append(content)\n",
    "\n",
    "    # Sources\n",
    "    sources = [s.text for s in container.find_all(\"span\", class_ = \"sourceTemplate\")]\n",
    "    all_sources.append(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae409df5",
   "metadata": {},
   "source": [
    "Vi har organiseret vores data ved at navngive kolonner og specificere, hvilken liste hver kolonne skal hente data fra. Herefter har vi benyttet Pandas til at lave en dataframe ud fra vores data. Til sidst eksporterer vi denne dataframe til hhv. csv og excel.\n",
    "\n",
    "Det færdige datasæt indeholder 2645 rækker samt 5 kolonner. Kolonnerne omfatter titler på hver enkelt artikel, deres urls/links, dato for, hvornår hver enkelt artikel er skrevet, de enkelte artiklers indhold dvs. selve artikelteksten samt den eller de kilder, den enkelte artikel er udsprunget af."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda1de7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:38:00.794758Z",
     "start_time": "2022-02-24T14:38:00.789770Z"
    }
   },
   "outputs": [],
   "source": [
    "# Organizing data\n",
    "data = {'Title': all_titles, 'Link': all_links, 'Date': all_dates, 'Content': all_content, 'Sources': all_sources}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b5bfa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title 2647\n",
      "Link 2647\n",
      "Date 2647\n",
      "Content 2647\n",
      "Sources 2647\n"
     ]
    }
   ],
   "source": [
    "for d in data.keys():\n",
    "    print(d,len(data[d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "976ea12c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-24T14:38:13.564425Z",
     "start_time": "2022-02-24T14:38:13.528011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframe from data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export data as csv and excel\n",
    "df.to_csv(\"wikinewsdata.csv\")\n",
    "df.to_excel(\"wikinewsdata.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a0b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
