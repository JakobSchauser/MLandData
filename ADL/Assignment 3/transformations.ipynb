{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"Running on\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = np.genfromtxt(\"mnist_train.csv\", delimiter = \",\", skip_header = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dat[:,1:]/255.0, dat[:,0]\n",
    "\n",
    "X = torch.tensor(X, requires_grad=True, dtype = torch.double).view(-1,28,28).unsqueeze(1)\n",
    "\n",
    "y = torch.tensor(y, requires_grad=False, dtype = int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 4\n",
    "plt.imshow(X[i].detach().numpy()[0])\n",
    "plt.title(str(y[i].numpy()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def onehot(y):\n",
    "#     out = torch.zeros(y.shape[0],10)\n",
    "#     for i in range(y.shape[0]):\n",
    "#         out[i][y[i]] = 1\n",
    "#     return out\n",
    "\n",
    "def batch_loader(X,y, batch_size = 6):\n",
    "    for i in range(y.shape[0]//batch_size):\n",
    "        X_out = X[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        y_out = y[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        # y_out = onehot(y_out)\n",
    "        \n",
    "        yield X_out, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,SIZE = 100, pretrain = -1):\n",
    "        super (CNN, self).__init__()\n",
    "\n",
    "        features = 10\n",
    "        if pretrain == 0:\n",
    "            features = 4\n",
    "        if pretrain == 1:\n",
    "            features = 2\n",
    "        if pretrain == 2:\n",
    "            features = 2\n",
    "        if pretrain == 3:\n",
    "            features = 2\n",
    "\n",
    "        self.SIZE = SIZE\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(2, 2))\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=SIZE, kernel_size=(5, 5))\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.linear1 = nn.Linear(in_features=SIZE*4*4, out_features=SIZE*4*4//2)\n",
    "        self.activation3 = nn.ReLU()\n",
    "\n",
    "        self.output = nn.Linear(in_features=SIZE*4*4//2, out_features = features)\n",
    "        # self.final_activation = nn.Softmax(dim=0)\n",
    "        self.final_activation = lambda x: x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation1(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.activation2(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(-1,self.SIZE*4*4)\n",
    "        x = self.activation3(self.linear1(x))\n",
    "        x = self.output(x)\n",
    "\n",
    "        out = self.final_activation(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def pretrain(self, x, _type = 0):\n",
    "        l = x.shape[0]\n",
    "        _x = x.clone().detach()\n",
    "\n",
    "        if _type == 0:\n",
    "            n = np.random.randint(0,4, l)\n",
    "\n",
    "            for i in range(l):\n",
    "                _x[i] = torch.rot90(_x[i], int(n[i]), [1,2])\n",
    "                \n",
    "            y = n\n",
    "            \n",
    "        if _type == 1:\n",
    "            c = np.random.random(l)<0.5\n",
    "\n",
    "            # wow\n",
    "            b = torch.tensor(c.astype(int)).unsqueeze(1).unsqueeze(2).unsqueeze(3).expand(16,-1,28,28)\n",
    "\n",
    "            noise = torch.randn_like(_x)*0.1 * b \n",
    "            _x = _x + noise\n",
    "            \n",
    "            y = c\n",
    "            \n",
    "        if _type == 2:\n",
    "            c = np.random.random(l)<0.5\n",
    "\n",
    "            noise = torch.roll(_x, 1, 0) * torch.tensor(c.astype(int)).unsqueeze(1).unsqueeze(2).unsqueeze(3).expand(16,-1,28,28)\n",
    "            _x = _x + noise\n",
    "            y = c      \n",
    "        \n",
    "        return _x, y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "def pretrain(model, n_epochs, pretrain_mode ):\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.0001)\n",
    "\n",
    "    model.train()\n",
    "    print(\"Pre-training\")\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "\n",
    "        c = 0\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        Loader = batch_loader(X,y, batch_size)\n",
    "        pbar = tqdm(Loader, total = y.shape[0]//batch_size)\n",
    "        losses = []\n",
    "        for dat, _ in pbar:\n",
    "            dat, label = model.pretrain(dat, pretrain_mode)\n",
    "            label = torch.tensor(label, dtype = torch.long)\n",
    "            \n",
    "            pred = model(dat.float())\n",
    "\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            corr = (label == pred.argmax(axis = 1))\n",
    "\n",
    "            epoch_acc += corr.float().mean()\n",
    "            c += 1\n",
    "\n",
    "            if c%10 == 0:\n",
    "                pbar.set_description(f\"loss {epoch_loss/c:.3} - acc {epoch_acc/c:.3}\")\n",
    "            # if c%100 == 0:\n",
    "                # print(pred[:5],label[:5],pred[:5].argmax(axis = 1) == label[:5])\n",
    "        print(f\"Epoch {i+1}/{n_epochs} - loss {epoch_loss/c:.3} - acc {epoch_acc/c:.3}\")\n",
    "    return losses\n",
    "            \n",
    "\n",
    "def fine_tune(model, n_epochs):\n",
    "    # for param in model.parameters():\n",
    "    #         param.requires_grad = False\n",
    "    print(\"Fine-tuning\")\n",
    "    model.output = nn.Linear(in_features = model.SIZE*4*4//2, out_features = 10)\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.0001)\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for i in range(n_epochs):\n",
    "        Loader = batch_loader(X,y, batch_size)\n",
    "\n",
    "        c = 0\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        pbar = tqdm(Loader, total = y.shape[0]//batch_size)\n",
    "        for dat, label in pbar:\n",
    "            pred = model(dat.float())\n",
    "\n",
    "            loss = loss_fn(pred, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss)\n",
    "            epoch_loss += loss\n",
    "            acc = (label == pred.argmax(axis = 1)).float().mean()\n",
    "            epoch_acc += acc\n",
    "            accuracies.append(acc)\n",
    "            c += 1\n",
    "\n",
    "            if c%10 == 0:\n",
    "                pbar.set_description(f\"loss {epoch_loss/c:.3} - acc {epoch_acc/c:.3}\")\n",
    "            # if c%100 == 0:\n",
    "                # print(pred[:5],label[:5],pred[:5].argmax(axis = 1) == label[:5])\n",
    "        print(f\"Epoch {i+1}/{n_epochs} - loss {epoch_loss/c:.3} - acc {epoch_acc/c:.3}\")\n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4315ef19b3be4d75834b422adbe76548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2 - loss 0.88 - acc 0.646\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4668cdcc8c1f4c98a969daa41089c0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - loss 0.545 - acc 0.806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46309479fdc349e186c5bfc29e1223bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-230-92de7eef5e2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpretrain_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfine_tune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-227-527791c1b483>\u001b[0m in \u001b[0;36mfine_tune\u001b[1;34m(model, n_epochs)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakob\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jakob\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for pretrain_mode in [0,1,2,3]:\n",
    "    print(\"Pretrain mode is\",pretrain_mode)\n",
    "    model = CNN(100, pretrain = pretrain_mode)\n",
    "    model.to(device)\n",
    "\n",
    "    pretrain_losses = pretrain(model, 10, pretrain_mode = pretrain_mode)\n",
    "\n",
    "    losses, accuracies = fine_tune(model, 10)\n",
    "    \n",
    "    run = {\"pretrain mode\": pretrain_mode,\"pretrain losses\" : pretrain_losses, \"fine-tune losses\" : losses, \"fine-tune accuracies\": accuracies}\n",
    "    np.save(\"pretain_data_\"+str(pretrain_mode)+\".npy\", run, allow_pickle = True)\n",
    "    print(\"finished!\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90ddd5eea33bf34e5045df3efad123ec5baf581e702e7e2d1915bea2be814a0a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
