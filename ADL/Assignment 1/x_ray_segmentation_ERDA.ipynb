{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiF9uQq63Gw9"
      },
      "source": [
        "# U-Net segmentation example\n",
        "### Advanced Deep Learning 2022\n",
        "This notebook was originally written by Mathias Perslev. It has been changed slightly by Christian Igel and subsequently slightly updated [Stefan Sommer](mailto:sommer@di.ku.dk).\n",
        "\n",
        "The U-Net is based on code written by Annika Brundyn and Akshay Kulkarni distributed as part of [*PyTorch Lightning Bolts*](https://pytorch-lightning.readthedocs.io/) under the  [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0.txt). \n",
        "\n",
        "We consider the data described in:\n",
        "\n",
        "Bram van Ginneken, Mikkel B. Stegmann, Marco Loog. [Segmentation of anatomical structures in chest radiographs using supervised methods: a comparative study on a public database](https://doi.org/10.1016/j.media.2005.02.002). *Medical Image Analysis* 10(1): 19-40, 2006\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k7dGxH23GxB"
      },
      "source": [
        "## Installs\n",
        "\n",
        "If some packages are missing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOnSWTac3GxC"
      },
      "outputs": [],
      "source": [
        "pip install -q pytorch_lightning lightning-bolts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0FulA5_3GxC"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp71Um2T3GxC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "from pprint import pformat\n",
        "from skimage.transform import resize\n",
        "#from sklearn.metrics import f1_score\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from matplotlib.pyplot import imread\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vzrPKEQWBGc"
      },
      "source": [
        "## Mount Google drive when using Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GoilmVm3Pts",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive/')\n",
        "    os.chdir('gdrive/MyDrive/ADL2022')\n",
        "except:\n",
        "    print('Google drive not mounted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yTYH2kVkeKK"
      },
      "source": [
        "## Test for GPU support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95WD78Nu7zYW"
      },
      "outputs": [],
      "source": [
        "# GPU support?\n",
        "gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if gpu else \"cpu\")\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHtp-0Rm3GxD"
      },
      "source": [
        "## U-Net\n",
        "Import the U-Net.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPU83L2i3GxD"
      },
      "outputs": [],
      "source": [
        "from pl_bolts.models.vision import UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oII1ZIcT3GxE"
      },
      "source": [
        "## Database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHFjgYdp3GxE"
      },
      "source": [
        "Load database with chest X-rays with lung segmentations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM393wDKXae-"
      },
      "outputs": [],
      "source": [
        "data_root='./datasets'\n",
        "data_npz='lung_field_dataset.npz'\n",
        "data_fn = os.path.join(data_root, \"lung_field_dataset.npz\")\n",
        "force_download = False\n",
        "\n",
        "if (not os.path.exists(data_fn)) or force_download:\n",
        "  download_url(\"https://sid.erda.dk/share_redirect/gCTc6o3KAh\", data_root, data_npz)\n",
        "else:\n",
        "  print('Using existing', data_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GBHpsfBZsvB"
      },
      "outputs": [],
      "source": [
        "def plot_image_with_segmentation(image, segmentation, ax=None):\n",
        "    \"\"\"\n",
        "    Plots an image with overlayed segmentation mask\n",
        "    \n",
        "    Returns: plt.fig and ax objects\n",
        "    \"\"\"\n",
        "    if ax is None:\n",
        "        fig = plt.figure(figsize=(8, 8))\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.axis(\"off\")\n",
        "    \n",
        "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
        "    mask = np.ma.masked_where(segmentation == 0, segmentation)\n",
        "    ax.imshow(mask.squeeze(), cmap=\"Set1\", alpha=0.5)\n",
        "    return plt.gcf(), ax\n",
        "\n",
        "\n",
        "def load_npz_dataset(path, keys=('x_train', 'y_train', 'x_val', 'y_val', 'x_test', 'y_test')):\n",
        "    archive = np.load(path)\n",
        "    return [archive.get(key) for key in keys]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr5tW2HWZveb"
      },
      "outputs": [],
      "source": [
        "# Load train/val/test data\n",
        "x_train, y_train, x_val, y_val, x_test, y_test = load_npz_dataset(data_fn)\n",
        "# Bring images into PyTorch format\n",
        "x_train = np.moveaxis(x_train, 3, 1)\n",
        "y_train = np.moveaxis(y_train, 3, 1)\n",
        "x_val = np.moveaxis(x_val, 3, 1)\n",
        "y_val = np.moveaxis(y_val, 3, 1)\n",
        "x_test = np.moveaxis(x_test, 3, 1)\n",
        "y_test = np.moveaxis(y_test, 3, 1)\n",
        "\n",
        "print(\"x train:\", x_train.shape)\n",
        "print(\"y train:\", y_train.shape)\n",
        "print(\"x val: \", x_val.shape)\n",
        "print(\"y val: \", y_val.shape)\n",
        "print(\"x test:\", x_test.shape)\n",
        "print(\"y test:\", y_test.shape)\n",
        "\n",
        "# Plot an example\n",
        "fig, ax = plot_image_with_segmentation(x_train[0], y_train[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isi0fnzy3GxG"
      },
      "source": [
        "### Init torch dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGzI4wyN3GxG"
      },
      "outputs": [],
      "source": [
        "def as_torch_dataset(x_arr, y_arr):\n",
        "    \"\"\"\n",
        "    Takes two numpy arrays of data points and labels (x_arr and y_arr, respectively) and\n",
        "    returns a torch TensorDataset object.\n",
        "    \n",
        "    Returns: torch.utils.data.TensorDataset\n",
        "    \"\"\"\n",
        "    dataset = torch.utils.data.TensorDataset(\n",
        "        torch.FloatTensor(x_arr), \n",
        "        torch.LongTensor(y_arr)\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "# Init torch datasets\n",
        "train_dataset = as_torch_dataset(x_train, y_train.squeeze(1))  # remove dummy channel dim\n",
        "val_dataset = as_torch_dataset(x_val, y_val.squeeze(1))\n",
        "test_dataset = as_torch_dataset(x_test, y_test.squeeze(1))\n",
        "\n",
        "# Init dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJtsklqj3GxH"
      },
      "source": [
        "### Function for evaluatingÂ a model on a data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzBstTq43GxH"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, metrics_dict, reduction=True, device=device):\n",
        "    \"\"\"\n",
        "    Evaluate a model 'model' on all batches of a torch DataLoader 'data_loader'.\n",
        "    On each batch, compute all metric functions stored in dictionary 'metrics_dict'.\n",
        "    \n",
        "    Returns: dict of metric_name: (list of batch-wise metrics if reduction == False, else single scalar)\n",
        "    \"\"\"\n",
        "    \n",
        "    # defaultdict(list) returns a dictionary-like object with default_factory list. \n",
        "    # When a new key is encountered, an entry is automatically created of type default_factory. \n",
        "    metrics = defaultdict(list)\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in data_loader:\n",
        "            # Predict on batch\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            logits = model(batch_x)\n",
        "            \n",
        "            # Compute all metrics\n",
        "            for metric_name, metric_func in metrics_dict.items():\n",
        "                value = metric_func(logits.cpu(), batch_y.cpu()).item() #.cpu().numpy()\n",
        "                metrics[metric_name].append(value)\n",
        "    \n",
        "    if reduction == True:\n",
        "        # Return mean values\n",
        "        return {key: np.mean(value) for key, value in metrics.items()}\n",
        "    else:\n",
        "        return metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPW7_p1c3GxH"
      },
      "source": [
        "## Main trainig loop function(s)\n",
        "\n",
        "We want to track the F1 score during training. This generates some additional code. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zpq0Qknp3GxH"
      },
      "outputs": [],
      "source": [
        "def run_one_epoch(model, loss, optimizer, train_loader, val_loader, n_epochs, metrics_dict, device=device):\n",
        "    \"\"\"\n",
        "    Run 1 epoch of training\n",
        "    Changes to model parameters and optimizer occour internally (state updates)\n",
        "    Returns:\n",
        "        two dictionaries, training and a validation metrics\n",
        "    \"\"\"\n",
        "    train_losses = []\n",
        "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "        # Zero out stored gradients for all parameters\n",
        "        batch_x = batch_x.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        print(f\"   -- Batch {i+1}/{len(train_loader)}\", end=\" / \")\n",
        "        # Predict on batch\n",
        "        logits = model(batch_x)\n",
        "        \n",
        "        # Compute loss function\n",
        "        loss_tensor = loss(logits, batch_y)\n",
        "        loss_scalar = loss_tensor.detach().cpu().numpy()\n",
        "        train_losses.append(loss_scalar)\n",
        "        print(\"Loss: \", loss_scalar)\n",
        "        \n",
        "        # Backprop and step\n",
        "        loss_tensor.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # Run validation\n",
        "    print(\"   Validation running...\")\n",
        "    val_metrics = evaluate_model(\n",
        "        model=model, \n",
        "        data_loader=val_loader,\n",
        "        metrics_dict=metrics_dict\n",
        "    )\n",
        "    # Return loss and metrics as dicts\n",
        "    return {\"loss\": np.mean(train_losses)}, val_metrics\n",
        "\n",
        "\n",
        "def merge_list_of_dicts(list_of_dicts):\n",
        "    \"\"\"\n",
        "    Takes a list of dictionaries and merges them into a single dictionary pointing to lists\n",
        "    \n",
        "    E.g. [{\"loss\": 5}, {\"loss\": 3}, {\"loss\": -2, \"F1\": 0.5}] --> {\"loss\": [5, 3, -2], \"F1\": [0.5]}\n",
        "    \n",
        "    Returns: dict\n",
        "    \"\"\"\n",
        "    merged = defaultdict(list)\n",
        "    for dict_ in list_of_dicts:\n",
        "        for value, key in dict_.items():\n",
        "            merged[value].append(key)\n",
        "    return merged\n",
        "\n",
        "\n",
        "def training_loop(model, loss, optimizer, train_loader, val_loader, n_epochs, init_epoch=None, metrics_dict=None, save_path=None):\n",
        "    \"\"\"\n",
        "    Run training of a model given a loss function, optimizer and a set of training and validation data.\n",
        "    Supports computing additional metrics on the validation set (only) via the metrics_dict param.\n",
        "    Specify save_path to store the model at each epoch.\n",
        "    \n",
        "    Returns: \n",
        "        Two lists of metric dictionaries for each epoch for training and validation, specifically\n",
        "    \"\"\"\n",
        "    train_history, val_history = [], []\n",
        "    \n",
        "    metrics_with_loss = {\"loss\": loss}\n",
        "    if metrics_dict is not None:\n",
        "        metrics_with_loss.update(metrics_dict)\n",
        "    \n",
        "    if init_epoch == None:\n",
        "        init_epoch = 0\n",
        "    try:\n",
        "        for i in range(init_epoch, n_epochs):\n",
        "            print(f\"Epoch {i+1}/{n_epochs}\")\n",
        "            train_metrics, val_metrics = run_one_epoch(\n",
        "                model=model, \n",
        "                loss=loss, \n",
        "                optimizer=optimizer, \n",
        "                train_loader=train_loader, \n",
        "                val_loader=val_loader, \n",
        "                n_epochs=n_epochs,\n",
        "                metrics_dict=metrics_with_loss\n",
        "            )\n",
        "            print(\"   Mean epoch metrics:\")\n",
        "            print(f\"   Training:   {pformat(train_metrics)}\")\n",
        "            print(f\"   Validation: {pformat(val_metrics)}\")\n",
        "            train_history.append(train_metrics), val_history.append(val_metrics)\n",
        "            \n",
        "            if save_path:\n",
        "                save_path_epoch = f\"epoch_{i+1}_{save_path}\"\n",
        "                print(f\"   Saving to: {save_path_epoch}\")\n",
        "                save_model(model, save_path_epoch, optimizer)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Training stopped.\")\n",
        "        pass\n",
        "    \n",
        "    # Merge list of training and validation dicts into single dicts    \n",
        "    return merge_list_of_dicts(train_history), merge_list_of_dicts(val_history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqzMchbT3GxH"
      },
      "source": [
        "## Function for plotting training/validation histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1trz-_Vn3GxH"
      },
      "outputs": [],
      "source": [
        "def plot_histories(train_history=None, val_history=None, label=\"Loss\"):\n",
        "    \"\"\"\n",
        "    Takes a list of training and/or validation metrics and plots them\n",
        "    Returns: plt.figure and ax objects\n",
        "    \"\"\"\n",
        "    if not train_history and not val_history:\n",
        "        raise ValueError(\"Must specify at least one of 'train_histories' and 'val_histories'\")\n",
        "    fig = plt.figure(figsize=(5, 3))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    epochs = np.arange(len(train_history or val_history))\n",
        "    if train_history:\n",
        "        ax.plot(epochs, train_history, label=\"Training\", color=\"black\")\n",
        "    if val_history:\n",
        "        ax.plot(epochs, val_history, label=\"Validation\", color=\"darkred\")\n",
        "    \n",
        "    ax.set_xlabel(\"Epoch\")\n",
        "    ax.set_ylabel(label)\n",
        "    ax.legend(loc=0)\n",
        "    \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ko2rho3GxI"
      },
      "source": [
        "## Functions for saving and loading model and optimizer state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVBXdhOo3GxI"
      },
      "outputs": [],
      "source": [
        "def save_model(model, path, optimizer=None):\n",
        "    \"\"\"\n",
        "    Saves the state_dict of a torch model and optional optimizer to 'path'\n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "    state = {\"model\": model.state_dict()}\n",
        "    if optimizer is not None:\n",
        "        state[\"optimizer\"] = optimizer.state_dict()\n",
        "    torch.save(state, path)\n",
        "\n",
        "\n",
        "def load_model(model, path, optimizer=None):\n",
        "    \"\"\"\n",
        "    Loads the state_dict of a torch model and optional optimizer from 'path'\n",
        "    Returns: None\n",
        "    \"\"\"\n",
        "    state = torch.load(path)\n",
        "    model.load_state_dict(state[\"model\"])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(state[\"optimizer\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8z2VvB83GxI"
      },
      "source": [
        "## Init U-Net model and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjrw4VsE3GxI"
      },
      "outputs": [],
      "source": [
        "# Init U-Net model\n",
        "model = UNet(\n",
        "    num_classes=2, \n",
        "    input_channels=1, \n",
        "    num_layers=4, \n",
        "    features_start=16,\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nXGVTLw3GxI"
      },
      "source": [
        "## Continue training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b2g5WUq3GxI"
      },
      "outputs": [],
      "source": [
        "# Specify integer, starting at 1\n",
        "init_epoch = None\n",
        "if init_epoch != None:\n",
        "    load_model(model, f\"epoch_{init_epoch}_model.ckpt\", optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrxO77vW3GxI"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWETbe5d3GxJ"
      },
      "outputs": [],
      "source": [
        "# Define loss and metrics\n",
        "loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "metrics = {\"f1\": torchmetrics.classification.F1Score(num_classes=2, average=\"macro\", mdmc_average=\"samplewise\")}\n",
        "\n",
        "# Run training\n",
        "train_history, val_history = training_loop(\n",
        "    model=model,\n",
        "    loss=loss, \n",
        "    optimizer=optimizer, \n",
        "    train_loader=train_loader, \n",
        "    val_loader=val_loader,\n",
        "    init_epoch=init_epoch,\n",
        "    n_epochs=100,\n",
        "    metrics_dict=metrics,\n",
        "    save_path=\"model.ckpt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WHHZc5K3GxJ"
      },
      "source": [
        "## Plot training and validation histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxBzn18s3GxJ"
      },
      "outputs": [],
      "source": [
        "plot_histories(train_history['loss'], val_history['loss'], label=\"Loss\")\n",
        "plot_histories(train_history=None, val_history=val_history['f1'], label=\"F1 Score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFD4Yrxy3GxJ"
      },
      "source": [
        "## Evaluate on single test-set image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncio8D4S3GxJ"
      },
      "outputs": [],
      "source": [
        "# Predict on a test image\n",
        "x_test, y_test = test_dataset[0]\n",
        "if gpu:\n",
        "  x_test = x_test.to(device)\n",
        "  y_test = y_test.to(device)\n",
        "pred = model(x_test.view(1, 1, x_test.shape[1], x_test.shape[2])).argmax(1).cpu().numpy()\n",
        "\n",
        "# Plot the result\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15, 15))\n",
        "ax1.set_title(\"True mask\")\n",
        "ax2.set_title(\"Predicted mask\")\n",
        "\n",
        "plot_image_with_segmentation(x_test.cpu(), y_test.cpu(), ax=ax1)\n",
        "plot_image_with_segmentation(x_test.cpu(), pred, ax=ax2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbWED8IJ3GxJ"
      },
      "source": [
        "## Evaluate on whole test-set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgPqlai_3GxK",
        "outputId": "582c497b-30a4-42ab-fd2c-f9e5c63f6f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test cases: 123\n",
            "Mean F1:    0.677644057002494\n",
            "STD  F1:    0.03900481769183917\n",
            "Min. F1:    0.5618932247161865\n"
          ]
        }
      ],
      "source": [
        "# OBS: Returns batch-wise metrics, but test_loader has batch_size = 1\n",
        "f1_test_scores = evaluate_model(model, test_loader, metrics, reduction=False)[\"f1\"]\n",
        "\n",
        "print(\"Test cases:\", len(f1_test_scores))\n",
        "print(\"Mean F1:   \", np.mean(f1_test_scores))\n",
        "print(\"STD  F1:   \", np.std(f1_test_scores))\n",
        "print(\"Min. F1:   \", np.min(f1_test_scores))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "x_ray_segmentation_ERDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}